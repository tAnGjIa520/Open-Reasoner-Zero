"""
å¿«é€Ÿè°ƒè¯•ç‰ˆæœ¬ - ä½¿ç”¨ transformers æ¨¡å‹ç”Ÿæˆå’Œåˆ†æ Learning Dynamics
(æ›¿ä»£ vLLMï¼Œç›´æ¥è·å–å®Œæ•´ vocab_size çš„ logits)
"""

import asyncio
import json
import os
import re
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field
from typing import List, Optional
from datetime import datetime

import numpy as np
import torch
import ray
from loguru import logger

# é…ç½®æ—¥å¿—æ–‡ä»¶è¾“å‡ºåˆ° orz_dynamic_debug_log ç›®å½•
log_dir = "orz_dynamic_debug_log"
os.makedirs(log_dir, exist_ok=True)
log_date = datetime.now().strftime("%Y%m%d")
logger.add(
    os.path.join(log_dir, f"eval_{log_date}.log"),
    rotation="00:00",  # æ¯å¤©åˆå¤œè½®è½¬
    retention="30 days",  # ä¿ç•™30å¤©
    level="INFO",
    encoding="utf-8",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}"
)

# å¯è§†åŒ–åº“
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯ï¼Œé€‚åˆæœåŠ¡å™¨ç¯å¢ƒ
import matplotlib.pyplot as plt
from matplotlib import rcParams

# è®¾ç½®ä¸­æ–‡å­—ä½“
rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
rcParams['axes.unicode_minus'] = False

from transformers import AutoModelForCausalLM, AutoTokenizer
from orz.ppo.tools.math_utils import is_equal, solution2answer
from orz.ppo.deepspeed_strategy import DeepspeedStrategy
from dataset.eval_dataset import EvalCustomDataset
from .extracted_get_batch_logps import analyze_learning_dynamics

# Global executor for async operations
executor = ThreadPoolExecutor(max_workers=64)


# ====================================================================
# ã€å¤š Checkpoint æ±‡æ€»å’Œå¯è§†åŒ–å‡½æ•°ã€‘
# ====================================================================

def extract_iter_from_path(checkpoint_path: str) -> int:
    """
    ä» checkpoint è·¯å¾„ä¸­æå– iter ç¼–å·

    ç¤ºä¾‹ï¼š
    "/path/to/orz_7b_ppo_jericho_1013/iter45/policy" â†’ 45
    "/path/to/iter90/policy" â†’ 90

    Args:
        checkpoint_path: checkpoint çš„å®Œæ•´è·¯å¾„

    Returns:
        iter ç¼–å·ï¼ˆintï¼‰ï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å› 0
    """
    import re
    match = re.search(r'iter(\d+)', checkpoint_path)
    if match:
        return int(match.group(1))
    return 0


def aggregate_multi_checkpoint_results(all_model_results: dict) -> dict:
    """
    èšåˆå¤šä¸ª checkpoint çš„è¯„ä¼°ç»“æœï¼ŒæŒ‰æ•°æ®é›†ã€æ ·æœ¬ç±»å‹ã€æŒ‡æ ‡ç»„ç»‡

    è¾“å…¥æ ¼å¼ï¼š
    {
        checkpoint_path_1: {
            'dataset1/ld_correct/prob_energy': 0.45,
            'dataset1/ld_correct/A_norm': 8.34,
            'dataset1/ld_incorrect/prob_energy': 0.65,
            ...
        },
        checkpoint_path_2: {...},
        ...
    }

    è¾“å‡ºæ ¼å¼ï¼š
    {
        'dataset1': {
            'correct': {
                'prob_energy': {'iters': [45, 90, ...], 'values': [0.45, 0.38, ...]},
                'A_norm': {'iters': [45, 90, ...], 'values': [8.34, 7.82, ...]},
                ...
            },
            'incorrect': {...},
            'random': {...}
        },
        'dataset2': {...},
        ...
    }

    Args:
        all_model_results: æ‰€æœ‰ checkpoint çš„è¯„ä¼°ç»“æœå­—å…¸

    Returns:
        èšåˆåçš„æ•°æ®ç»“æ„
    """
    aggregated = {}

    # æå–æ‰€æœ‰çš„ (dataset_name, sample_type, metric_name)
    for checkpoint_path, results in all_model_results.items():
        iter_num = extract_iter_from_path(checkpoint_path)

        for key, value in results.items():
            # è§£æ key: "dataset_name/ld_sample_type/metric_name" æˆ– "dataset_name/ld_sample_type/count"
            parts = key.split('/')
            if len(parts) != 3:
                continue

            dataset_name, ld_prefix, metric_or_count = parts

            # è·³è¿‡ count å’Œå…¶ä»–éæŒ‡æ ‡é¡¹
            if metric_or_count == 'count':
                continue

            # è§£ææ ·æœ¬ç±»å‹ï¼šld_correct â†’ correct
            if not ld_prefix.startswith('ld_'):
                continue
            sample_type = ld_prefix[3:]  # å»æ‰ 'ld_' å‰ç¼€
            metric_name = metric_or_count

            # åˆå§‹åŒ–æ•°æ®ç»“æ„
            if dataset_name not in aggregated:
                aggregated[dataset_name] = {}
            if sample_type not in aggregated[dataset_name]:
                aggregated[dataset_name][sample_type] = {}
            if metric_name not in aggregated[dataset_name][sample_type]:
                aggregated[dataset_name][sample_type][metric_name] = {
                    'iters': [],
                    'values': []
                }

            # æ·»åŠ æ•°æ®ç‚¹
            aggregated[dataset_name][sample_type][metric_name]['iters'].append(iter_num)
            aggregated[dataset_name][sample_type][metric_name]['values'].append(float(value))

    # æŒ‰ iter æ’åº
    for dataset_name in aggregated:
        for sample_type in aggregated[dataset_name]:
            for metric_name in aggregated[dataset_name][sample_type]:
                data = aggregated[dataset_name][sample_type][metric_name]
                # æŒ‰ iter æ’åº
                sorted_pairs = sorted(zip(data['iters'], data['values']), key=lambda x: x[0])
                if sorted_pairs:
                    data['iters'], data['values'] = zip(*sorted_pairs)
                    data['iters'] = list(data['iters'])
                    data['values'] = list(data['values'])

    return aggregated


def visualize_multi_checkpoint_trends(aggregated_data: dict, output_dir: str = "orz_dynamic_debug_log"):
    """
    ä¸ºæ¯ä¸ªæ•°æ®é›†ç”ŸæˆæŠ˜çº¿å›¾å¯¹æ¯”ï¼Œæ˜¾ç¤ºæŒ‡æ ‡åœ¨ä¸åŒ checkpoint ä¸Šçš„å˜åŒ–è¶‹åŠ¿

    å¯¹æ¯ä¸ªæ•°æ®é›†ï¼š
    - ç”Ÿæˆä¸€ä¸ªå¤§å›¾ï¼ˆ2x3 ç½‘æ ¼æˆ–å…¶ä»–å¸ƒå±€ï¼‰
    - åŒ…å«æ‰€æœ‰ 5 ä¸ªæ ¸å¿ƒæŒ‡æ ‡
    - æ¯ä¸ªæŒ‡æ ‡å­å›¾æ˜¾ç¤º 3 æ¡æŠ˜çº¿ï¼ˆcorrect, incorrect, randomï¼‰
    - x è½´ä¸º iter ç¼–å·ï¼Œy è½´ä¸ºæŒ‡æ ‡å€¼

    Args:
        aggregated_data: aggregate_multi_checkpoint_results çš„è¾“å‡º
        output_dir: è¾“å‡ºç›®å½•
    """
    import math

    os.makedirs(output_dir, exist_ok=True)

    # å®šä¹‰æŒ‡æ ‡å’Œæ˜¾ç¤ºåç§°
    metrics = ['out_token', 'out_argmax', 'A_norm', 'prob_gap2_mean', 'prob_energy']
    metric_labels = {
        'out_token': 'True Label Log Prob',
        'out_argmax': 'Argmax Token Log Prob',
        'A_norm': 'Output Vector Norm',
        'prob_gap2_mean': 'Prediction-Label Gap',
        'prob_energy': 'Pull-up Energy',
    }

    # æ ·æœ¬ç±»å‹é…ç½®
    sample_types = ['correct', 'incorrect', 'random']
    colors = {
        'correct': '#2ecc71',    # ç»¿è‰²
        'incorrect': '#e74c3c',  # çº¢è‰²
        'random': '#3498db',     # è“è‰²
    }

    # å¯¹æ¯ä¸ªæ•°æ®é›†ç”Ÿæˆä¸€å¼ å¤§å›¾
    for dataset_name, dataset_data in aggregated_data.items():
        logger.info(f"Generating trend visualization for {dataset_name}...")

        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
        has_data = False
        for sample_type in sample_types:
            if sample_type in dataset_data and dataset_data[sample_type]:
                has_data = True
                break

        if not has_data:
            logger.warning(f"Dataset {dataset_name} has no valid data, skipping visualization")
            continue

        # è®¡ç®—ç½‘æ ¼å¸ƒå±€ï¼š5 ä¸ªæŒ‡æ ‡ï¼Œé€‰æ‹© 2x3 æˆ– 3x2
        num_metrics = len(metrics)
        ncols = 3
        nrows = math.ceil(num_metrics / ncols)

        fig, axes = plt.subplots(nrows, ncols, figsize=(18, 5*nrows))

        # ç¡®ä¿ axes æ˜¯ 2D æ•°ç»„
        if nrows == 1:
            axes = axes.reshape(1, -1)
        elif ncols == 1:
            axes = axes.reshape(-1, 1)

        axes = axes.flatten()

        # ç»˜åˆ¶æ¯ä¸ªæŒ‡æ ‡
        for idx, metric_name in enumerate(metrics):
            ax = axes[idx]

            # è·å–è¯¥æŒ‡æ ‡çš„æ•°æ®
            has_metric_data = False
            for sample_type in sample_types:
                if (sample_type in dataset_data and
                    metric_name in dataset_data[sample_type] and
                    dataset_data[sample_type][metric_name]['values']):

                    data = dataset_data[sample_type][metric_name]
                    iters = data['iters']
                    values = data['values']

                    ax.plot(iters, values,
                           marker='o',
                           label=sample_type.capitalize(),
                           color=colors[sample_type],
                           linewidth=2.5,
                           markersize=8,
                           alpha=0.8)
                    has_metric_data = True

            # è®¾ç½®å­å›¾æ ‡é¢˜å’Œæ ‡ç­¾
            ax.set_title(f'{metric_labels[metric_name]}', fontsize=12, fontweight='bold')
            ax.set_xlabel('Iter', fontsize=11, fontweight='bold')
            ax.set_ylabel('Value', fontsize=11, fontweight='bold')
            ax.grid(True, alpha=0.3, linestyle='--')
            ax.legend(fontsize=10, loc='best')

            # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œæ˜¾ç¤ºç©ºç™½
            if not has_metric_data:
                ax.text(0.5, 0.5, 'No Data', ha='center', va='center',
                       transform=ax.transAxes, fontsize=12, color='gray')
                ax.set_xlim(0, 1)
                ax.set_ylim(0, 1)

        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(num_metrics, len(axes)):
            axes[idx].axis('off')

        # è®¾ç½®æ€»æ ‡é¢˜
        fig.suptitle(f'Learning Dynamics Trends - {dataset_name}',
                    fontsize=16, fontweight='bold', y=0.995)

        plt.tight_layout(rect=[0, 0, 1, 0.99])

        # ä¿å­˜å›¾è¡¨åˆ° figures å­ç›®å½•
        figures_dir = os.path.join(output_dir, "figures")
        os.makedirs(figures_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        figure_path = os.path.join(figures_dir, f"ld_trends_{dataset_name}_{timestamp}.png")
        plt.savefig(figure_path, dpi=150, bbox_inches='tight')
        plt.close()

        logger.info(f"Trend visualization saved to: {figure_path}")


@dataclass
class EvaluatorConfig:
    """JSONL åˆ†æä¸“ç”¨é…ç½®ç±»"""
    # Model and tokenizer
    model_path: str
    tokenizer_path: Optional[str] = None

    # JSONL æ–‡ä»¶è·¯å¾„å’Œåˆ†æè®¾ç½®
    jsonl_file: str = ""  # JSONL æ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«å·²ç”Ÿæˆçš„æ¨¡å‹è¾“å‡º
    num_correct_samples: int = 10   # åˆ†æçš„æ­£ç¡®æ ·æœ¬æ•°
    num_incorrect_samples: int = 10 # åˆ†æçš„é”™è¯¯æ ·æœ¬æ•°

    # Output settings
    output_dir: str = "orz_dynamic_debug_log"

    # Visualization settings
    enable_visualization: bool = True  # æ˜¯å¦å¯ç”¨å¤š checkpoint è¶‹åŠ¿å¯è§†åŒ–

    def __post_init__(self):
        if self.tokenizer_path is None:
            self.tokenizer_path = self.model_path


class Evaluator:
    """å¿«é€Ÿè°ƒè¯•ç‰ˆè¯„ä¼°å™¨ - ä½¿ç”¨ transformers"""

    def __init__(
        self,
        config: Optional[EvaluatorConfig] = None,
        model_path: Optional[str] = None,
        model=None,
        tokenizer=None,
        **kwargs
    ):
        """åˆå§‹åŒ– JSONL åˆ†æè¯„ä¼°å™¨"""
        # Initialize Ray if not already done
        if not ray.is_initialized():
            ray.init()

        # å¤„ç†é…ç½®å¯¹è±¡
        if config is not None:
            self.cfg = config
        else:
            if model_path is None and model is None:
                raise ValueError("å¿…é¡»æŒ‡å®š model_path æˆ– model")

            config_kwargs = {
                "model_path": model_path or "dummy_path",
            }
            config_kwargs.update(kwargs)
            self.cfg = EvaluatorConfig(**config_kwargs)

        self.tokenizer = tokenizer
        self.model = model
        self.executor = executor
        self._user_provided_model = model
        self._user_provided_tokenizer = tokenizer

        logger.info(f"Initializing JSONL Analyzer with config: {self.cfg}")

        # Load model and tokenizer only
        if not self._user_provided_tokenizer:
            self._load_tokenizer()
        if not self._user_provided_model:
            self._load_model()

        logger.info("JSONL Analyzer initialization completed")

    def _load_tokenizer(self):
        """Load tokenizer from pretrained model"""
        logger.info(f"Loading tokenizer from {self.cfg.tokenizer_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.cfg.tokenizer_path,
            trust_remote_code=True,
        )

    def _load_model(self):
        """Load transformers model"""
        logger.info(f"Loading transformers model from {self.cfg.model_path}")
        self.model = AutoModelForCausalLM.from_pretrained(
            self.cfg.model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
        )
        self.model.eval()
        logger.info("Model loaded successfully")

    def _sample_random_tokens(self, logits, labels, token_ids, num_samples=5):
        """
        ä»ç”Ÿæˆçš„åºåˆ—ä¸­éšæœºé‡‡æ · token åŠå…¶å¯¹åº”çš„ logits

        Args:
            logits: torch.FloatTensor, shape (1, seq_len, vocab_size)
            labels: torch.LongTensor, shape (1, seq_len)
            token_ids: list of generated token IDs
            num_samples: é‡‡æ ·çš„ token æ•°é‡ï¼ˆé»˜è®¤ 5ï¼‰

        Returns:
            sampled_data: å­—å…¸ï¼ŒåŒ…å«é‡‡æ ·çš„ token ä¿¡æ¯
        """
        import random

        if logits is None or labels is None or token_ids is None:
            return None

        seq_len = len(token_ids)
        valid_positions = [i for i in range(seq_len) if i < labels.shape[1] and labels[0, i] != -100]

        if len(valid_positions) == 0:
            return None

        actual_samples = min(num_samples, len(valid_positions))
        sampled_positions = sorted(random.sample(valid_positions, actual_samples))

        sampled_data = {
            'sample_positions': sampled_positions,
            'sample_tokens': [],
            'sample_logprobs': [],
            'sample_token_names': [],
            'sample_token_probs': [],
        }

        for pos in sampled_positions:
            token_id = int(token_ids[pos])
            logit_vector = logits[0, pos, :].detach().cpu()
            log_probs = torch.nn.functional.log_softmax(logit_vector, dim=-1)
            probs = torch.softmax(logit_vector, dim=-1)
            token_logprob = log_probs[token_id].item()
            token_prob = probs[token_id].item()

            try:
                token_name = self.tokenizer.decode([token_id]).strip()
            except:
                token_name = f"<unk_token_{token_id}>"

            sampled_data['sample_tokens'].append(token_id)
            sampled_data['sample_logprobs'].append(token_logprob)
            sampled_data['sample_token_names'].append(token_name)
            sampled_data['sample_token_probs'].append(token_prob)

        return sampled_data

    async def _analyze_custom_jsonl(self):
        """
        åˆ†æJSONLæ ¼å¼çš„å·²ç”Ÿæˆè¯„ä¼°è¾“å‡º

        JSONLæ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰ï¼š
        {
            "prompt": "å®Œæ•´çš„promptæ–‡æœ¬",
            "output": "æ¨¡å‹ç”Ÿæˆçš„å®Œæ•´è¾“å‡º",
            "final_answer": "ä»è¾“å‡ºä¸­æå–çš„ç­”æ¡ˆ",
            "answer": "ground truthç­”æ¡ˆ",
            "iscorrect": true/false
        }

        æµç¨‹ï¼š
        1. è¯»å–JSONLæ–‡ä»¶
        2. æ ¹æ®iscorrectå­—æ®µåˆ†ç±»ä¸ºæ­£ç¡®å’Œé”™è¯¯æ ·æœ¬
        3. æ”¶é›†æŒ‡å®šæ•°é‡çš„æ­£ç¡®å’Œé”™è¯¯æ ·æœ¬
        4. å¯¹æ¯ä¸ªæ ·æœ¬ï¼š
           - Tokenize prompt å’Œ output
           - å¯¹outputä¸­æ¯ä¸ªä½ç½® t è¿›è¡Œè‡ªå›å½’å‰å‘ä¼ æ’­
           - æ‹¼æ¥æ‰€æœ‰logits â†’ (1, output_len, vocab_size)
           - æ„é€ labelsï¼ˆpromptéƒ¨åˆ†maskä¸º-100ï¼‰
           - è°ƒç”¨analyze_learning_dynamicsåˆ†æ
        5. ä¸ºæ­£ç¡®/é”™è¯¯æ ·æœ¬åˆ†åˆ«æ±‡æ€»ç»“æœ
        """
        if self.cfg.jsonl_file is None or self.cfg.jsonl_file == "":
            return None

        ld_jsonl_results = {
            'correct': [],
            'incorrect': []
        }

        try:
            logger.info(f"Loading JSONL file: {self.cfg.jsonl_file}")

            # Step 1: è¯»å–JSONLæ–‡ä»¶ï¼Œåˆ†ç±»æ ·æœ¬
            correct_samples = []
            incorrect_samples = []

            with open(self.cfg.jsonl_file, 'r', encoding='utf-8') as f:
                for line_idx, line in enumerate(f):
                    if line.strip() == '':
                        continue

                    try:
                        data = json.loads(line)

                        # æ£€æŸ¥å¿…è¦å­—æ®µ
                        if 'prompt' not in data or 'output' not in data:
                            logger.warning(f"Line {line_idx}: Missing 'prompt' or 'output' field, skipping")
                            continue

                        # æ£€æŸ¥æ˜¯å¦æä¾›äº†correctnessä¿¡æ¯
                        is_correct = data.get('iscorrect', None)

                        if is_correct is True:
                            correct_samples.append(data)
                        elif is_correct is False:
                            incorrect_samples.append(data)
                        else:
                            logger.warning(f"Line {line_idx}: 'iscorrect' field not found or not boolean, skipping")
                            continue

                        # æ£€æŸ¥æ˜¯å¦å·²æ”¶é›†è¶³å¤Ÿçš„æ ·æœ¬
                        if (len(correct_samples) >= self.cfg.num_correct_samples and
                            len(incorrect_samples) >= self.cfg.num_incorrect_samples):
                            break

                    except json.JSONDecodeError as e:
                        logger.warning(f"Line {line_idx}: Failed to parse JSON: {e}")
                        continue

            logger.info(f"Loaded {len(correct_samples)} correct samples, {len(incorrect_samples)} incorrect samples")

            # Step 2: å¤„ç†æ­£ç¡®æ ·æœ¬
            if len(correct_samples) > 0:
                logger.info(f"Analyzing {min(len(correct_samples), self.cfg.num_correct_samples)} correct samples...")
                for sample_idx, sample in enumerate(correct_samples[:self.cfg.num_correct_samples]):
                    try:
                        ld_metrics = self._process_jsonl_sample(sample, f"correct_{sample_idx}")
                        if ld_metrics:
                            ld_jsonl_results['correct'].append(ld_metrics)
                    except Exception as e:
                        logger.error(f"Failed to analyze correct sample {sample_idx}: {e}")
                        continue

            # Step 3: å¤„ç†é”™è¯¯æ ·æœ¬
            if len(incorrect_samples) > 0:
                logger.info(f"Analyzing {min(len(incorrect_samples), self.cfg.num_incorrect_samples)} incorrect samples...")
                for sample_idx, sample in enumerate(incorrect_samples[:self.cfg.num_incorrect_samples]):
                    try:
                        ld_metrics = self._process_jsonl_sample(sample, f"incorrect_{sample_idx}")
                        if ld_metrics:
                            ld_jsonl_results['incorrect'].append(ld_metrics)
                    except Exception as e:
                        logger.error(f"Failed to analyze incorrect sample {sample_idx}: {e}")
                        continue

            logger.info("="*80 + "\n")
            return ld_jsonl_results

        except FileNotFoundError:
            logger.error(f"JSONL file not found: {self.cfg.custom_sentence_jsonl_file}")
            return None
        except Exception as e:
            logger.error(f"Failed to analyze JSONL file: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _process_jsonl_sample(self, sample: dict, sample_id: str) -> Optional[dict]:
        """
        å¤„ç†å•ä¸ªJSONLæ ·æœ¬çš„Learning Dynamicsåˆ†æ

        Args:
            sample: JSONLä¸­çš„å•ä¸ªæ ·æœ¬
            sample_id: æ ·æœ¬IDï¼ˆç”¨äºæ—¥å¿—ï¼‰

        Returns:
            LDæŒ‡æ ‡å­—å…¸ï¼Œæˆ–Noneå¦‚æœå¤„ç†å¤±è´¥
        """
        try:
            prompt = sample['prompt']
            output = sample['output']

            logger.info(f"Processing sample: {sample_id}")
            logger.info(f"Prompt: {prompt[:100]}...")
            logger.info(f"Output: {output[:100]}...")

            # Step 1: Tokenize
            prompt_inputs = self.tokenizer(prompt, return_tensors="pt")
            prompt_ids = prompt_inputs["input_ids"][0].tolist()

            output_inputs = self.tokenizer(output, return_tensors="pt", add_special_tokens=False)
            output_ids = output_inputs["input_ids"][0].tolist()

            logger.info(f"Prompt tokens: {len(prompt_ids)}, Output tokens: {len(output_ids)}")

            # Step 2: è‡ªå›å½’è®¡ç®—æ¯ä¸ªoutputä½ç½®çš„logits
            logger.info("Computing logits via autoregressive forward pass...")
            all_logits = []

            for t in range(len(output_ids)):
                # æ„é€ è¾“å…¥ï¼šprompt + output[:t]
                input_ids = prompt_ids + output_ids[:t]
                input_tensor = torch.tensor([input_ids]).to(self.model.device)

                # Forward passï¼Œè·å–æœ€åä¸€ä¸ªä½ç½®çš„logitsï¼ˆé¢„æµ‹output[t]ï¼‰
                with torch.no_grad():
                    outputs = self.model(input_ids=input_tensor)
                    logits_at_t = outputs.logits[0, -1, :]  # (vocab_size,)

                all_logits.append(logits_at_t.cpu())

            logger.info(f"Computed logits for {len(all_logits)} positions")

            # Step 3: æ‹¼æ¥logitsåˆ°å®Œæ•´åºåˆ—ä½ç½®
            full_seq_len = len(prompt_ids) + len(output_ids)
            # ä½¿ç”¨å®é™… logits çš„ vocab_sizeï¼Œè€Œä¸æ˜¯ tokenizer æŠ¥å‘Šçš„ vocab_size
            # å› ä¸ºæ¨¡å‹å¯èƒ½ç»è¿‡äº† token embedding resize ç­‰æ“ä½œ
            actual_vocab_size = all_logits[0].shape[0] if len(all_logits) > 0 else self.tokenizer.vocab_size
            logits_full = torch.zeros(1, full_seq_len, actual_vocab_size)

            # å°†outputçš„logitsæ”¾åˆ°å¯¹åº”ä½ç½®ï¼ˆpromptéƒ¨åˆ†ä¸º0ï¼‰
            for t, logit in enumerate(all_logits):
                logits_full[0, len(prompt_ids) + t, :] = logit

            # Step 4: æ„é€ labels
            labels = torch.tensor([prompt_ids + output_ids], dtype=torch.long)
            # Mask promptéƒ¨åˆ†
            labels[:, :len(prompt_ids)] = -100

            logger.info(f"Logits shape: {logits_full.shape}, Labels shape: {labels.shape}")

            # Step 5: Analyze Learning Dynamics
            ld_result = analyze_learning_dynamics(
                logits=logits_full,
                labels=labels,
                tokenizer=self.tokenizer,
                verbose=False
            )

            # æå–å…³é”®æŒ‡æ ‡
            metrics = {
                'sample_id': sample_id,
                'prompt_len': len(prompt_ids),
                'output_len': len(output_ids),
                'out_token': float(ld_result['per_sample']['out_token'][0]),
                'out_argmax': float(ld_result['per_sample']['out_argmax'][0]),
                'A_norm': float(ld_result['per_sample']['A_norm'].squeeze()),
                'prob_gap2_mean': float(ld_result['per_sample']['prob_gap2_mean'][0]),
                'prob_energy': float(ld_result['per_sample']['prob_energy'][0]),
            }

            logger.info(f"âœ“ Completed analysis for {sample_id}")
            return metrics

        except Exception as e:
            logger.error(f"Error processing sample {sample_id}: {e}")
            import traceback
            traceback.print_exc()
            return None

    async def eval(self) -> dict:
        """JSONL æ–‡ä»¶åˆ†æè¯„ä¼°"""
        log_dict = defaultdict(float)

        logger.info("="*80)
        logger.info("ğŸ“„ Analyzing JSONL file")
        logger.info(f"File: {self.cfg.jsonl_file}")
        logger.info(f"Correct samples: {self.cfg.num_correct_samples}, Incorrect samples: {self.cfg.num_incorrect_samples}")
        logger.info("="*80)

        # åˆ†æJSONLæ–‡ä»¶
        ld_jsonl_results = await self._analyze_custom_jsonl()

        # å¤„ç†å’Œæ±‡æ€»ç»“æœ - è°ƒæ•´æ ¼å¼ä»¥é€‚é…å¤šcheckpointæ±‡æ€»
        if ld_jsonl_results:
            # ä¸ºæ¯ä¸ªæ ·æœ¬ç”Ÿæˆ checkpoint/dataset/type/metric æ ¼å¼çš„ key
            # è¿™æ ·å¯ä»¥è¢« aggregate_multi_checkpoint_results æ­£ç¡®å¤„ç†
            if ld_jsonl_results['correct']:
                for metric in ['out_token', 'out_argmax', 'A_norm', 'prob_gap2_mean', 'prob_energy']:
                    values = [s[metric] for s in ld_jsonl_results['correct']]
                    if values:
                        log_dict[f"jsonl_dataset/ld_correct/{metric}"] = values
                        log_dict[f"jsonl_dataset_correct_{metric}_mean"] = float(np.mean(values))
                        log_dict[f"jsonl_dataset_correct_{metric}_std"] = float(np.std(values))

            if ld_jsonl_results['incorrect']:
                for metric in ['out_token', 'out_argmax', 'A_norm', 'prob_gap2_mean', 'prob_energy']:
                    values = [s[metric] for s in ld_jsonl_results['incorrect']]
                    if values:
                        log_dict[f"jsonl_dataset/ld_incorrect/{metric}"] = values
                        log_dict[f"jsonl_dataset_incorrect_{metric}_mean"] = float(np.mean(values))
                        log_dict[f"jsonl_dataset_incorrect_{metric}_std"] = float(np.std(values))

            # è¾“å‡ºç»Ÿè®¡æ€»ç»“
            logger.info("\n" + "="*80)
            logger.info("ã€JSONL Analysis Summaryã€‘")
            logger.info("="*80 + "\n")

            if ld_jsonl_results['correct']:
                logger.info(f"\nã€Correct Samples (n={len(ld_jsonl_results['correct'])})ã€‘")
                logger.info(f"  out_token: {float(np.mean([s['out_token'] for s in ld_jsonl_results['correct']])):.4f}")
                logger.info(f"  out_argmax: {float(np.mean([s['out_argmax'] for s in ld_jsonl_results['correct']])):.4f}")
                logger.info(f"  A_norm: {float(np.mean([s['A_norm'] for s in ld_jsonl_results['correct']])):.4f}")
                logger.info(f"  prob_gap2_mean: {float(np.mean([s['prob_gap2_mean'] for s in ld_jsonl_results['correct']])):.4f}")
                logger.info(f"  prob_energy: {float(np.mean([s['prob_energy'] for s in ld_jsonl_results['correct']])):.4f}")

            if ld_jsonl_results['incorrect']:
                logger.info(f"\nã€Incorrect Samples (n={len(ld_jsonl_results['incorrect'])})ã€‘")
                logger.info(f"  out_token: {float(np.mean([s['out_token'] for s in ld_jsonl_results['incorrect']])):.4f}")
                logger.info(f"  out_argmax: {float(np.mean([s['out_argmax'] for s in ld_jsonl_results['incorrect']])):.4f}")
                logger.info(f"  A_norm: {float(np.mean([s['A_norm'] for s in ld_jsonl_results['incorrect']])):.4f}")
                logger.info(f"  prob_gap2_mean: {float(np.mean([s['prob_gap2_mean'] for s in ld_jsonl_results['incorrect']])):.4f}")
                logger.info(f"  prob_energy: {float(np.mean([s['prob_energy'] for s in ld_jsonl_results['incorrect']])):.4f}")
            logger.info("\n" + "="*80 + "\n")

        return dict(log_dict)

    def _visualize_learning_dynamics(self, ld_correct_samples, ld_incorrect_samples, ld_random_tokens=None, dataset_name="All"):
        """
        Generate comparison bar chart for learning dynamics metrics

        Args:
            ld_correct_samples: æ­£ç¡®ç­”æ¡ˆçš„å­¦ä¹ åŠ¨æ€æ•°æ®
            ld_incorrect_samples: é”™è¯¯ç­”æ¡ˆçš„å­¦ä¹ åŠ¨æ€æ•°æ®
            ld_random_tokens: éšæœºé‡‡æ ·tokençš„å­¦ä¹ åŠ¨æ€æ•°æ®ï¼ˆå¯é€‰ï¼‰
            dataset_name: æ•°æ®é›†åç§°ï¼Œç”¨äºå›¾è¡¨æ ‡é¢˜å’Œæ–‡ä»¶å
        """
        # Check if there is data
        if not ld_correct_samples['out_token'] and not ld_incorrect_samples['out_token']:
            logger.warning(f"No learning dynamics data to visualize for {dataset_name}")
            return None

        metrics = ['out_token', 'out_argmax', 'A_norm', 'prob_gap2_mean', 'prob_energy']
        metric_labels = {
            'out_token': 'True Label\nLog Probability',
            'out_argmax': 'Argmax Token\nLog Probability',
            'A_norm': 'Output Vector\nNorm',
            'prob_gap2_mean': 'Prediction-Label\nGap',
            'prob_energy': 'Pull-up Energy\n(Correction Strength)',
        }

        correct_means = []
        correct_stds = []
        incorrect_means = []
        incorrect_stds = []
        random_means = []
        random_stds = []

        for metric in metrics:
            if ld_correct_samples[metric]:
                correct_means.append(float(np.mean(ld_correct_samples[metric])))
                correct_stds.append(float(np.std(ld_correct_samples[metric])))
            else:
                correct_means.append(0)
                correct_stds.append(0)

            if ld_incorrect_samples[metric]:
                incorrect_means.append(float(np.mean(ld_incorrect_samples[metric])))
                incorrect_stds.append(float(np.std(ld_incorrect_samples[metric])))
            else:
                incorrect_means.append(0)
                incorrect_stds.append(0)

            # éšæœº tokens æ•°æ®ï¼ˆå¯é€‰ï¼‰
            if ld_random_tokens and ld_random_tokens[metric]:
                random_means.append(float(np.mean(ld_random_tokens[metric])))
                random_stds.append(float(np.std(ld_random_tokens[metric])))
            else:
                random_means.append(0)
                random_stds.append(0)

        fig, ax = plt.subplots(figsize=(16, 8))
        x = np.arange(len(metrics))

        # è°ƒæ•´å®½åº¦ä»¥æ”¯æŒ 3 ç»„æ•°æ®
        if ld_random_tokens and ld_random_tokens['out_token']:
            width = 0.25  # 3 ç»„æŸ±å­
            bars1 = ax.bar(x - width, correct_means, width, label='Correct Answers',
                           color='#2ecc71', alpha=0.8, edgecolor='black', linewidth=1.5,
                           yerr=correct_stds, capsize=5, error_kw={'linewidth': 1.5})
            bars2 = ax.bar(x, incorrect_means, width, label='Incorrect Answers',
                           color='#e74c3c', alpha=0.8, edgecolor='black', linewidth=1.5,
                           yerr=incorrect_stds, capsize=5, error_kw={'linewidth': 1.5})
            bars3 = ax.bar(x + width, random_means, width, label='Random Tokens',
                           color='#3498db', alpha=0.8, edgecolor='black', linewidth=1.5,
                           yerr=random_stds, capsize=5, error_kw={'linewidth': 1.5})

            def add_value_labels(bars):
                for bar in bars:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                           f'{height:.3f}',
                           ha='center', va='bottom', fontsize=9, fontweight='bold')

            add_value_labels(bars1)
            add_value_labels(bars2)
            add_value_labels(bars3)
        else:
            # åªæœ‰ 2 ç»„æ•°æ®
            width = 0.35
            bars1 = ax.bar(x - width/2, correct_means, width, label='Correct Answers',
                           color='#2ecc71', alpha=0.8, edgecolor='black', linewidth=1.5,
                           yerr=correct_stds, capsize=5, error_kw={'linewidth': 1.5})
            bars2 = ax.bar(x + width/2, incorrect_means, width, label='Incorrect Answers',
                           color='#e74c3c', alpha=0.8, edgecolor='black', linewidth=1.5,
                           yerr=incorrect_stds, capsize=5, error_kw={'linewidth': 1.5})

            def add_value_labels(bars):
                for bar in bars:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                           f'{height:.3f}',
                           ha='center', va='bottom', fontsize=10, fontweight='bold')

            add_value_labels(bars1)
            add_value_labels(bars2)

        ax.set_xlabel('Learning Dynamics Metrics', fontsize=14, fontweight='bold')
        ax.set_ylabel('Metric Value', fontsize=14, fontweight='bold')
        ax.set_title(f'Learning Dynamics Comparison - {dataset_name}', fontsize=16, fontweight='bold', pad=20)
        ax.set_xticks(x)
        ax.set_xticklabels([metric_labels[m] for m in metrics], fontsize=11)
        ax.legend(fontsize=12, loc='upper left')
        ax.grid(axis='y', alpha=0.3, linestyle='--')

        correct_count = len(ld_correct_samples['out_token']) if ld_correct_samples['out_token'] else 0
        incorrect_count = len(ld_incorrect_samples['out_token']) if ld_incorrect_samples['out_token'] else 0
        random_count = len(ld_random_tokens['out_token']) if ld_random_tokens and ld_random_tokens['out_token'] else 0

        if random_count > 0:
            info_text = f'Correct Samples: {correct_count}  |  Incorrect Samples: {incorrect_count}  |  Random Tokens: {random_count}'
        else:
            info_text = f'Correct Samples: {correct_count}  |  Incorrect Samples: {incorrect_count}'

        fig.text(0.5, 0.02, info_text, ha='center', fontsize=11, style='italic')

        plt.tight_layout(rect=[0, 0.03, 1, 1])

        figures_dir = os.path.join(self.cfg.output_dir, "figures")
        os.makedirs(figures_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        figure_path = os.path.join(figures_dir, f"ld_comparison_{dataset_name}_{timestamp}.png")
        plt.savefig(figure_path, dpi=150, bbox_inches='tight')
        plt.close()

        logger.info(f"Learning Dynamics visualization saved to: {figure_path}")
        return figure_path

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("Cleaning up evaluator resources")
        if hasattr(self, 'model') and self.model is not None:
            del self.model
            torch.cuda.empty_cache()
        if ray.is_initialized():
            ray.shutdown()
        logger.info("Cleanup completed")

if __name__ == "__main__":
    import argparse

    # Debug evaluation mode - å¿«é€Ÿæµ‹è¯•ï¼ˆä½¿ç”¨ transformersï¼‰
    logger.info("Running in DEBUG evaluation mode (TRANSFORMERS)")
    logger.info("This is a fast debug version with minimal samples")
    logger.info("Using transformers for generation (not vLLM)")

    # Parse command line arguments
    parser = argparse.ArgumentParser(description="JSONL Learning Dynamics Analyzer")
    parser.add_argument("--checkpoint_paths", nargs="+", default=None, help="Checkpoint paths to evaluate")
    parser.add_argument("--model_path", type=str, default=None, help="Model path (alternative to checkpoint_paths)")
    parser.add_argument("--jsonl_file", type=str, required=True, help="JSONL file path containing pre-generated evaluation outputs")
    parser.add_argument("--num_correct_samples", type=int, default=10, help="Number of correct samples to analyze")
    parser.add_argument("--num_incorrect_samples", type=int, default=10, help="Number of incorrect samples to analyze")
    parser.add_argument("--output_dir", type=str, default="eval_results_debug", help="Output directory for results")
    parser.add_argument("--log_dir", type=str, default="orz_dynamic_debug_log", help="Log directory name")

    args = parser.parse_args()

    # Reconfigure logger with user-provided log_dir
    log_dir = args.log_dir
    os.makedirs(log_dir, exist_ok=True)
    log_date = datetime.now().strftime("%Y%m%d")
    # Remove existing handlers and add new one with the custom log_dir
    logger.remove()  # Remove all existing handlers
    logger.add(
        os.path.join(log_dir, f"eval_{log_date}.log"),
        rotation="00:00",  # æ¯å¤©åˆå¤œè½®è½¬
        retention="30 days",  # ä¿ç•™30å¤©
        level="INFO",
        encoding="utf-8",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}"
    )
    logger.info(f"Logger reconfigured with log_dir: {log_dir}")

    # Determine checkpoint paths
    if args.checkpoint_paths:
        checkpoint_path_list = args.checkpoint_paths
    elif args.model_path:
        checkpoint_path_list = [args.model_path]
    else:
        checkpoint_path_list = [
            "/mnt/shared-storage-user/tangjia/orz_7b_ppo_jericho_1013/iter0/policy",
            "/mnt/shared-storage-user/tangjia/orz_7b_ppo_jericho_1013/iter45/policy",
            "/mnt/shared-storage-user/tangjia/orz_7b_ppo_jericho_1013/iter90/policy",
            "/mnt/shared-storage-user/tangjia/orz_7b_ppo_jericho_1013/iter180/policy",
            # å¯ä»¥æ·»åŠ æ›´å¤š checkpoint è·¯å¾„
        ]

    all_model_results = {}

    for checkpoint_path in checkpoint_path_list:
        logger.info(f"\n{'='*80}")
        logger.info(f"ã€Processing: {checkpoint_path}ã€‘")
        logger.info(f"{'='*80}\n")

        eval_config = EvaluatorConfig(
            model_path=checkpoint_path,
            tokenizer_path=checkpoint_path,
            jsonl_file=args.jsonl_file,
            num_correct_samples=args.num_correct_samples,
            num_incorrect_samples=args.num_incorrect_samples,
            output_dir=args.output_dir,
            enable_visualization=True,
        )
        evaluator = Evaluator(eval_config)

        try:
            results = asyncio.run(evaluator.eval())
            logger.info(f"Debug evaluation results for {checkpoint_path}: {results}")
            all_model_results[checkpoint_path] = results
        finally:
            evaluator.cleanup()

    # æ±‡æ€»æ‰€æœ‰ç»“æœ
    logger.info(f"\n{'='*80}")
    logger.info(f"ã€All Model Results Summaryã€‘")
    logger.info(f"{'='*80}\n")
    for checkpoint_path, results in all_model_results.items():
        logger.info(f"\nã€{checkpoint_path}ã€‘")
        for key, value in results.items():
            logger.info(f"  {key}: {value}")

    logger.info("Multi-Checkpoint Learning Dynamics Trend Analysis")

    # Aggregate results
    logger.info("Aggregating results from multiple checkpoints...")
    aggregated_data = aggregate_multi_checkpoint_results(all_model_results)

    # Output summary statistics
    logger.info("Aggregated data statistics:")
    for dataset_name, dataset_data in aggregated_data.items():
        logger.info(f"  Dataset: {dataset_name}")
        for sample_type in ['correct', 'incorrect', 'random']:
            if sample_type in dataset_data and dataset_data[sample_type]:
                num_metrics = len(dataset_data[sample_type])
                logger.info(f"    {sample_type}: {num_metrics} metrics")

    # Generate trend visualization
    logger.info("Generating multi-checkpoint trends visualization...")
    visualize_multi_checkpoint_trends(aggregated_data, output_dir="orz_dynamic_debug_log")

    logger.info("All evaluations and visualizations completed!")


