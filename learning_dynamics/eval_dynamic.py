"""
ç‹¬ç«‹è¯„ä¼°æ¨¡å— - ç”¨äºè¯„ä¼°å·²è®­ç»ƒçš„æ¨¡å‹
"""

import asyncio
import json
import os
import re
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field
from typing import List, Optional
from datetime import datetime

import numpy as np
import torch
import ray
from loguru import logger

# é…ç½®æ—¥å¿—æ–‡ä»¶è¾“å‡ºåˆ° orz_dynamic_log ç›®å½•
log_dir = "orz_dynamic_log"
os.makedirs(log_dir, exist_ok=True)
log_date = datetime.now().strftime("%Y%m%d")
logger.add(
    os.path.join(log_dir, f"eval_{log_date}.log"),
    rotation="00:00",  # æ¯å¤©åˆå¤œè½®è½¬
    retention="30 days",  # ä¿ç•™30å¤©
    level="INFO",
    encoding="utf-8",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}"
)

# å¯è§†åŒ–åº“
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯ï¼Œé€‚åˆæœåŠ¡å™¨ç¯å¢ƒ
import matplotlib.pyplot as plt
from matplotlib import rcParams

# è®¾ç½®ä¸­æ–‡å­—ä½“
rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
rcParams['axes.unicode_minus'] = False

from transformers import AutoModelForCausalLM, AutoTokenizer
from orz.ppo.tools.math_utils import is_equal, solution2answer
from orz.ppo.deepspeed_strategy import DeepspeedStrategy
from dataset.eval_dataset import EvalCustomDataset
from .extracted_get_batch_logps import analyze_learning_dynamics

# Global executor for async operations
executor = ThreadPoolExecutor(max_workers=64)


# ====================================================================
# ã€å¤š Checkpoint æ±‡æ€»å’Œå¯è§†åŒ–å‡½æ•°ã€‘
# ====================================================================

def extract_iter_from_path(checkpoint_path: str) -> int:
    """
    ä» checkpoint è·¯å¾„ä¸­æå– iter ç¼–å·

    ç¤ºä¾‹ï¼š
    "/path/to/orz_7b_ppo_jericho_1013/iter45/policy" â†’ 45
    "/path/to/iter90/policy" â†’ 90

    Args:
        checkpoint_path: checkpoint çš„å®Œæ•´è·¯å¾„

    Returns:
        iter ç¼–å·ï¼ˆintï¼‰ï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å› 0
    """
    match = re.search(r'iter(\d+)', checkpoint_path)
    if match:
        return int(match.group(1))
    return 0


def aggregate_multi_checkpoint_results(all_model_results: dict) -> dict:
    """
    èšåˆå¤šä¸ª checkpoint çš„è¯„ä¼°ç»“æœï¼ŒæŒ‰æ•°æ®é›†ã€æ ·æœ¬ç±»å‹ã€æŒ‡æ ‡ç»„ç»‡

    è¾“å…¥æ ¼å¼ï¼š
    {
        checkpoint_path_1: {
            'dataset1/ld_correct/prob_energy': 0.45,
            'dataset1/ld_correct/A_norm': 8.34,
            'dataset1/ld_incorrect/prob_energy': 0.65,
            ...
        },
        checkpoint_path_2: {...},
        ...
    }

    è¾“å‡ºæ ¼å¼ï¼š
    {
        'dataset1': {
            'correct': {
                'prob_energy': {'iters': [45, 90, ...], 'values': [0.45, 0.38, ...]},
                'A_norm': {'iters': [45, 90, ...], 'values': [8.34, 7.82, ...]},
                ...
            },
            'incorrect': {...},
            'random': {...}
        },
        'dataset2': {...},
        ...
    }

    Args:
        all_model_results: æ‰€æœ‰ checkpoint çš„è¯„ä¼°ç»“æœå­—å…¸

    Returns:
        èšåˆåçš„æ•°æ®ç»“æ„
    """
    aggregated = {}

    # æå–æ‰€æœ‰çš„ (dataset_name, sample_type, metric_name)
    for checkpoint_path, results in all_model_results.items():
        iter_num = extract_iter_from_path(checkpoint_path)

        for key, value in results.items():
            # è§£æ key: "dataset_name/ld_sample_type/metric_name" æˆ– "dataset_name/ld_sample_type/count"
            parts = key.split('/')
            if len(parts) != 3:
                continue

            dataset_name, ld_prefix, metric_or_count = parts

            # è·³è¿‡ count å’Œå…¶ä»–éæŒ‡æ ‡é¡¹
            if metric_or_count == 'count':
                continue

            # è§£ææ ·æœ¬ç±»å‹ï¼šld_correct â†’ correct
            if not ld_prefix.startswith('ld_'):
                continue
            sample_type = ld_prefix[3:]  # å»æ‰ 'ld_' å‰ç¼€
            metric_name = metric_or_count

            # åˆå§‹åŒ–æ•°æ®ç»“æ„
            if dataset_name not in aggregated:
                aggregated[dataset_name] = {}
            if sample_type not in aggregated[dataset_name]:
                aggregated[dataset_name][sample_type] = {}
            if metric_name not in aggregated[dataset_name][sample_type]:
                aggregated[dataset_name][sample_type][metric_name] = {
                    'iters': [],
                    'values': []
                }

            # æ·»åŠ æ•°æ®ç‚¹
            aggregated[dataset_name][sample_type][metric_name]['iters'].append(iter_num)
            aggregated[dataset_name][sample_type][metric_name]['values'].append(float(value))

    # æŒ‰ iter æ’åº
    for dataset_name in aggregated:
        for sample_type in aggregated[dataset_name]:
            for metric_name in aggregated[dataset_name][sample_type]:
                data = aggregated[dataset_name][sample_type][metric_name]
                # æŒ‰ iter æ’åº
                sorted_pairs = sorted(zip(data['iters'], data['values']), key=lambda x: x[0])
                if sorted_pairs:
                    data['iters'], data['values'] = zip(*sorted_pairs)
                    data['iters'] = list(data['iters'])
                    data['values'] = list(data['values'])

    return aggregated


def visualize_multi_checkpoint_trends(aggregated_data: dict, output_dir: str = "eval_results"):
    """
    ä¸ºæ¯ä¸ªæ•°æ®é›†ç”ŸæˆæŠ˜çº¿å›¾å¯¹æ¯”ï¼Œæ˜¾ç¤ºæŒ‡æ ‡åœ¨ä¸åŒ checkpoint ä¸Šçš„å˜åŒ–è¶‹åŠ¿

    å¯¹æ¯ä¸ªæ•°æ®é›†ï¼š
    - ç”Ÿæˆä¸€ä¸ªå¤§å›¾ï¼ˆ2x3 ç½‘æ ¼æˆ–å…¶ä»–å¸ƒå±€ï¼‰
    - åŒ…å«æ‰€æœ‰ 5 ä¸ªæ ¸å¿ƒæŒ‡æ ‡
    - æ¯ä¸ªæŒ‡æ ‡å­å›¾æ˜¾ç¤º 3 æ¡æŠ˜çº¿ï¼ˆcorrect, incorrect, randomï¼‰
    - x è½´ä¸º iter ç¼–å·ï¼Œy è½´ä¸ºæŒ‡æ ‡å€¼

    Args:
        aggregated_data: aggregate_multi_checkpoint_results çš„è¾“å‡º
        output_dir: è¾“å‡ºç›®å½•
    """
    import math

    os.makedirs(output_dir, exist_ok=True)

    # å®šä¹‰æŒ‡æ ‡å’Œæ˜¾ç¤ºåç§°
    metrics = ['out_token', 'out_argmax', 'A_norm', 'prob_gap2_mean', 'prob_energy']
    metric_labels = {
        'out_token': 'True Label Log Prob',
        'out_argmax': 'Argmax Token Log Prob',
        'A_norm': 'Output Vector Norm',
        'prob_gap2_mean': 'Prediction-Label Gap',
        'prob_energy': 'Pull-up Energy',
    }

    # æ ·æœ¬ç±»å‹é…ç½®
    sample_types = ['correct', 'incorrect', 'random']
    colors = {
        'correct': '#2ecc71',    # ç»¿è‰²
        'incorrect': '#e74c3c',  # çº¢è‰²
        'random': '#3498db',     # è“è‰²
    }

    # å¯¹æ¯ä¸ªæ•°æ®é›†ç”Ÿæˆä¸€å¼ å¤§å›¾
    for dataset_name, dataset_data in aggregated_data.items():
        logger.info(f"Generating trend visualization for {dataset_name}...")

        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
        has_data = False
        for sample_type in sample_types:
            if sample_type in dataset_data and dataset_data[sample_type]:
                has_data = True
                break

        if not has_data:
            logger.warning(f"Dataset {dataset_name} has no valid data, skipping visualization")
            continue

        # è®¡ç®—ç½‘æ ¼å¸ƒå±€ï¼š5 ä¸ªæŒ‡æ ‡ï¼Œé€‰æ‹© 2x3 æˆ– 3x2
        num_metrics = len(metrics)
        ncols = 3
        nrows = math.ceil(num_metrics / ncols)

        fig, axes = plt.subplots(nrows, ncols, figsize=(18, 5*nrows))

        # ç¡®ä¿ axes æ˜¯ 2D æ•°ç»„
        if nrows == 1:
            axes = axes.reshape(1, -1)
        elif ncols == 1:
            axes = axes.reshape(-1, 1)

        axes = axes.flatten()

        # ç»˜åˆ¶æ¯ä¸ªæŒ‡æ ‡
        for idx, metric_name in enumerate(metrics):
            ax = axes[idx]

            # è·å–è¯¥æŒ‡æ ‡çš„æ•°æ®
            has_metric_data = False
            for sample_type in sample_types:
                if (sample_type in dataset_data and
                    metric_name in dataset_data[sample_type] and
                    dataset_data[sample_type][metric_name]['values']):

                    data = dataset_data[sample_type][metric_name]
                    iters = data['iters']
                    values = data['values']

                    ax.plot(iters, values,
                           marker='o',
                           label=sample_type.capitalize(),
                           color=colors[sample_type],
                           linewidth=2.5,
                           markersize=8,
                           alpha=0.8)
                    has_metric_data = True

            # è®¾ç½®å­å›¾æ ‡é¢˜å’Œæ ‡ç­¾
            ax.set_title(f'{metric_labels[metric_name]}', fontsize=12, fontweight='bold')
            ax.set_xlabel('Iter', fontsize=11, fontweight='bold')
            ax.set_ylabel('Value', fontsize=11, fontweight='bold')
            ax.grid(True, alpha=0.3, linestyle='--')
            ax.legend(fontsize=10, loc='best')

            # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œæ˜¾ç¤ºç©ºç™½
            if not has_metric_data:
                ax.text(0.5, 0.5, 'No Data', ha='center', va='center',
                       transform=ax.transAxes, fontsize=12, color='gray')
                ax.set_xlim(0, 1)
                ax.set_ylim(0, 1)

        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(num_metrics, len(axes)):
            axes[idx].axis('off')

        # è®¾ç½®æ€»æ ‡é¢˜
        fig.suptitle(f'Learning Dynamics Trends - {dataset_name}',
                    fontsize=16, fontweight='bold', y=0.995)

        plt.tight_layout(rect=[0, 0, 1, 0.99])

        # ä¿å­˜å›¾è¡¨åˆ° figures å­ç›®å½•
        figures_dir = os.path.join(output_dir, "figures")
        os.makedirs(figures_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        figure_path = os.path.join(figures_dir, f"ld_trends_{dataset_name}_{timestamp}.png")
        plt.savefig(figure_path, dpi=150, bbox_inches='tight')
        plt.close()

        logger.info(f"Trend visualization saved to: {figure_path}")

    logger.info("=" * 80)


@dataclass
class EvaluatorConfig:
    """ç‹¬ç«‹è¯„ä¼°é…ç½®ç±»"""
    # Model and tokenizer
    model_path: str  # checkpoint path or HF model name
    tokenizer_path: Optional[str] = None  # if None, use model_path

    # Generation settings
    temperature: float = 1.0
    top_p: float = 1.0
    top_k: int = -1
    generate_max_len: int = 8000
    do_sample: bool = True

    # Data settings
    eval_prompt_data: List[str] = field(default_factory=lambda: [
        "data/eval_data/eval_jericho_dataset_his10_4games_1.8k_20251013_instruct.json",
        "data/eval_data/math500.json",
        "data/eval_data/aime2024.json",
        "data/eval_data/gpqa_diamond.json",
    ])
    prompt_max_len: int = 2048
    max_eval_samples: int = 5000  # æ¯ä¸ªæ•°æ®é›†çš„æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°

    # Learning Dynamics æ ·æœ¬æ”¶é›†è®¾ç½®
    min_correct_samples: int = 30   # æ­£ç¡®ç­”æ¡ˆçš„æœ€å°æ ·æœ¬æ•°
    min_incorrect_samples: int = 10 # é”™è¯¯ç­”æ¡ˆçš„æœ€å°æ ·æœ¬æ•°
    min_random_samples: int = 10    # éšæœºé‡‡æ ·tokençš„æœ€å°æ ·æœ¬æ•°

    # Output settings
    output_dir: str = "orz_dynamic_log"
    log_dir: str = "orz_dynamic_log"  # Log directory name
    save_detailed_results: bool = True

    # Visualization settings
    enable_visualization: bool = True  # æ˜¯å¦å¯ç”¨å¯è§†åŒ–

    # Custom analysis settings
    custom_sentence_files: Optional[List[str]] = None  # JSONæ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡ä»¶åŒ…å«{"prompt": "...", "response": "..."}
    verbose_learning_dynamics: bool = False     # è¯¦ç»†è¾“å‡ºlearning dynamicsåˆ†æç»“æœ

    def __post_init__(self):
        if self.tokenizer_path is None:
            self.tokenizer_path = self.model_path


class Evaluator:
    """ç‹¬ç«‹è¯„ä¼°ç±»ï¼Œæ”¯æŒä»checkpointåŠ è½½æ¨¡å‹è¿›è¡Œè¯„ä¼°"""

    def __init__(
        self,
        config: Optional[EvaluatorConfig] = None,
        model_path: Optional[str] = None,
        eval_prompt_data: Optional[List[str]] = None,
        model=None,
        tokenizer=None,
        **kwargs
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        æ”¯æŒå¤šç§åˆå§‹åŒ–æ–¹å¼ï¼š
        1. ä¼ å…¥ EvaluatorConfig å¯¹è±¡ï¼ˆä¿ç•™åŸæœ‰æ–¹å¼ï¼‰
           Evaluator(config=EvaluatorConfig(...))

        2. ä¼ å…¥å¿…è¦å‚æ•°ï¼Œå…¶ä»–ä½¿ç”¨é»˜è®¤å€¼
           Evaluator(model_path="...", eval_prompt_data=[...])

        3. ä¼ å…¥å·²åŠ è½½çš„æ¨¡å‹å¯¹è±¡
           Evaluator(model=my_model, tokenizer=my_tokenizer, eval_prompt_data=[...])

        Args:
            config: EvaluatorConfig é…ç½®å¯¹è±¡ï¼ˆå¯é€‰ï¼‰
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼Œå½“ä¸ä¼  config æ—¶ä½¿ç”¨ï¼‰
            eval_prompt_data: è¯„ä¼°æ•°æ®è·¯å¾„åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            model: å·²åŠ è½½çš„ transformers æ¨¡å‹å¯¹è±¡ï¼ˆå¯é€‰ï¼‰
            tokenizer: å·²åŠ è½½çš„ tokenizer å¯¹è±¡ï¼ˆå¯é€‰ï¼‰
            **kwargs: å…¶ä»– EvaluatorConfig å‚æ•°
        """
        # Initialize Ray if not already done
        if not ray.is_initialized():
            ray.init()

        # å¤„ç†é…ç½®å¯¹è±¡
        if config is not None:
            # ä½¿ç”¨ä¼ å…¥çš„ config å¯¹è±¡
            self.cfg = config
        else:
            # ä»å‚æ•°æ„å»º config å¯¹è±¡
            if model_path is None and model is None:
                raise ValueError("å¿…é¡»æŒ‡å®š model_path æˆ– model")

            config_kwargs = {
                "model_path": model_path or "dummy_path",  # å½“ä½¿ç”¨é¢„åŠ è½½æ¨¡å‹æ—¶ï¼Œå¯ä»¥æ˜¯å ä½ç¬¦
                "eval_prompt_data": eval_prompt_data or [
                    "data/eval_data/math500.json",
                    "data/eval_data/aime2024.json",
                    "data/eval_data/gpqa_diamond.json",
                ],
            }
            # åˆå¹¶å…¶ä»– kwargs
            config_kwargs.update(kwargs)
            self.cfg = EvaluatorConfig(**config_kwargs)

        self.tokenizer = tokenizer
        self.model = model
        self.eval_dataset = None
        self.executor = executor
        self._user_provided_model = model
        self._user_provided_tokenizer = tokenizer

        logger.info(f"Initializing Evaluator with config: {self.cfg}")

        # Load components
        if not self._user_provided_tokenizer:
            self._load_tokenizer()
        if not self._user_provided_model:
            self._load_model()
        self._load_eval_datasets()

        logger.info("Evaluator initialization completed")

    def _load_tokenizer(self):
        """Load tokenizer from pretrained model"""
        logger.info(f"Loading tokenizer from {self.cfg.tokenizer_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.cfg.tokenizer_path,
            trust_remote_code=True,
        )

    def _load_model(self):
        """Load transformers model"""
        logger.info(f"Loading transformers model from {self.cfg.model_path}")
        self.model = AutoModelForCausalLM.from_pretrained(
            self.cfg.model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
        )
        self.model.eval()
        logger.info("Model loaded successfully")

    def _load_eval_datasets(self):
        """Load evaluation datasets"""
        logger.info(f"Loading evaluation datasets from {self.cfg.eval_prompt_data}")
        dialogues = []
        for file_path in self.cfg.eval_prompt_data:
            logger.info(f"Loading dataset from {file_path}")
            with open(file_path, "r") as f:
                loaded_data = json.load(f)
                for item in loaded_data:
                    # Add file name as metadata
                    item["file_name"] = os.path.splitext(os.path.basename(file_path))[0]
                dialogues.extend(loaded_data)

        logger.info(f"Loaded {len(dialogues)} samples from evaluation datasets")
        logger.info(f"Sample collection strategy: åŠ¨æ€æ”¶é›†ï¼Œç›´åˆ°å„ç±»åˆ«è¾¾åˆ°æœ€å°æ ·æœ¬æ•°æˆ–è¾¾åˆ°å…¨å±€ä¸Šé™")
        logger.info(f"  - Min correct samples: {self.cfg.min_correct_samples}")
        logger.info(f"  - Min incorrect samples: {self.cfg.min_incorrect_samples}")
        logger.info(f"  - Min random samples: {self.cfg.min_random_samples}")
        logger.info(f"  - Max evaluation samples per dataset: {self.cfg.max_eval_samples}")

        # Create strategy object for dataset processing
        strategy = DeepspeedStrategy()

        self.eval_dataset = EvalCustomDataset(
            dialogues,
            self.tokenizer,
            self.cfg.prompt_max_len,
            strategy,
            pretrain_mode=False,
            num_processors=1,
        )
        logger.info(f"Processed {len(self.eval_dataset)} evaluation samples")

    async def _analyze_custom_sentence(self):
        """
        åˆ†æç”¨æˆ·æä¾›çš„å¤šä¸ªå®Œæ•´è¯­å¥ï¼Œè‡ªå›å½’è®¡ç®—æ¯ä¸ªä½ç½®çš„logits

        æ–‡ä»¶æ ¼å¼ (JSON):
        {
            "prompt": "Solve this problem: ...",
            "response": "The answer is ..."
        }

        æµç¨‹ï¼š
        1. å¾ªç¯å¤„ç†æ¯ä¸ªè‡ªå®šä¹‰å¥å­æ–‡ä»¶
        2. å¯¹æ¯ä¸ªæ–‡ä»¶ï¼š
           - è¯»å–JSONæ–‡ä»¶
           - Tokenize prompt å’Œ response
           - å¯¹responseä¸­æ¯ä¸ªä½ç½® t è¿›è¡Œè‡ªå›å½’å‰å‘ä¼ æ’­
           - æ‹¼æ¥æ‰€æœ‰logits â†’ (1, response_len, vocab_size)
           - æ„é€ labelsï¼ˆpromptéƒ¨åˆ†maskä¸º-100ï¼‰
           - è°ƒç”¨analyze_learning_dynamicsåˆ†æ
        3. ä¸ºæ¯ä¸ªæ–‡ä»¶å•ç‹¬è¾“å‡ºç»“æœ
        """
        if self.cfg.custom_sentence_files is None or len(self.cfg.custom_sentence_files) == 0:
            return

        ld_custom_sentences = {}

        for file_path in self.cfg.custom_sentence_files:
            try:
                logger.info("\n" + "="*80)
                logger.info(f"ã€Processing: {os.path.basename(file_path)}ã€‘")
                logger.info("="*80)

                # Step 1: åŠ è½½JSONæ–‡ä»¶
                logger.info(f"Loading custom sentence from: {file_path}")
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                if 'prompt' not in data or 'response' not in data:
                    logger.error("JSON file must contain 'prompt' and 'response' fields")
                    continue

                prompt = data['prompt']
                response = data['response']
                logger.info(f"Prompt: {prompt[:100]}...")
                logger.info(f"Response: {response[:100]}...")

                # Step 2: Tokenize
                prompt_inputs = self.tokenizer(prompt, return_tensors="pt")
                prompt_ids = prompt_inputs["input_ids"][0].tolist()

                response_inputs = self.tokenizer(response, return_tensors="pt", add_special_tokens=False)
                response_ids = response_inputs["input_ids"][0].tolist()

                logger.info(f"Prompt tokens: {len(prompt_ids)}")
                logger.info(f"Response tokens: {len(response_ids)}")

                # Step 3: è‡ªå›å½’è®¡ç®—æ¯ä¸ªresponseä½ç½®çš„logits
                logger.info("Computing logits via autoregressive forward pass...")
                all_logits = []

                for t in range(len(response_ids)):
                    # æ„é€ è¾“å…¥ï¼šprompt + response[:t]
                    input_ids = prompt_ids + response_ids[:t]
                    input_tensor = torch.tensor([input_ids]).to(self.model.device)

                    # Forward passï¼Œè·å–æœ€åä¸€ä¸ªä½ç½®çš„logitsï¼ˆé¢„æµ‹response[t]ï¼‰
                    with torch.no_grad():
                        outputs = self.model(input_ids=input_tensor)
                        logits_at_t = outputs.logits[0, -1, :]  # (vocab_size,)

                    all_logits.append(logits_at_t.cpu())

                logger.info(f"Computed logits for {len(all_logits)} positions")

                # Step 5: æ‹¼æ¥logitsåˆ°å®Œæ•´åºåˆ—ä½ç½®
                full_seq_len = len(prompt_ids) + len(response_ids)
                logits_full = torch.zeros(1, full_seq_len, self.tokenizer.vocab_size)

                # å°†responseçš„logitsæ”¾åˆ°å¯¹åº”ä½ç½®ï¼ˆpromptéƒ¨åˆ†ä¸º0ï¼‰
                for t, logit in enumerate(all_logits):
                    logits_full[0, len(prompt_ids) + t, :] = logit

                # Step 6: æ„é€ labels
                labels = torch.tensor([prompt_ids + response_ids], dtype=torch.long)
                # Mask promptéƒ¨åˆ†
                labels[:, :len(prompt_ids)] = -100

                logger.info(f"Logits shape: {logits_full.shape}")
                logger.info(f"Labels shape: {labels.shape}")

                # Step 7: åˆ†æLearning Dynamics
                logger.info("\n" + "="*80)
                logger.info(f"ã€Custom Sentence Learning Dynamics Analysis - {os.path.basename(file_path)}ã€‘")
                logger.info("="*80 + "\n")

                ld_result = analyze_learning_dynamics(
                    logits=logits_full,
                    labels=labels,
                    tokenizer=self.tokenizer,
                    verbose=self.cfg.verbose_learning_dynamics
                )

                # æå–å…³é”®æŒ‡æ ‡
                ld_custom_sentences[os.path.splitext(os.path.basename(file_path))[0]] = {
                    'out_token': ld_result['per_sample']['out_token'][0],
                    'out_argmax': ld_result['per_sample']['out_argmax'][0],
                    'A_norm': float(ld_result['per_sample']['A_norm'].squeeze()),
                    'prob_gap2_mean': ld_result['per_sample']['prob_gap2_mean'][0],
                    'prob_energy': ld_result['per_sample']['prob_energy'][0],
                }

                logger.info("="*80 + "\n")

            except FileNotFoundError:
                logger.error(f"Custom sentence file not found: {file_path}")
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse JSON file: {e}")
            except Exception as e:
                logger.error(f"Failed to analyze custom sentence: {e}")
                import traceback
                traceback.print_exc()

        return ld_custom_sentences

    def _sample_random_tokens(self, logits, labels, token_ids, num_samples=5):
        """
        ä»ç”Ÿæˆçš„åºåˆ—ä¸­éšæœºé‡‡æ · token åŠå…¶å¯¹åº”çš„ logits

        Args:
            logits: torch.FloatTensor, shape (1, seq_len, vocab_size)
            labels: torch.LongTensor, shape (1, seq_len)
            token_ids: list of generated token IDs
            num_samples: é‡‡æ ·çš„ token æ•°é‡ï¼ˆé»˜è®¤ 5ï¼‰

        Returns:
            sampled_data: å­—å…¸ï¼ŒåŒ…å«é‡‡æ ·çš„ token ä¿¡æ¯
                {
                    'sample_positions': [pos1, pos2, ...],
                    'sample_tokens': [token_id1, token_id2, ...],
                    'sample_logprobs': [logprob1, logprob2, ...],
                    'sample_token_names': ['token_name1', 'token_name2', ...],
                    'sample_token_probs': [prob1, prob2, ...],
                }
        """
        import random

        if logits is None or labels is None or token_ids is None:
            return None

        seq_len = len(token_ids)

        # æœ‰æ•ˆä½ç½®æ•°ï¼ˆé -100 maskï¼‰
        valid_positions = [i for i in range(seq_len) if i < labels.shape[1] and labels[0, i] != -100]

        if len(valid_positions) == 0:
            return None

        # ç¡®å®šå®é™…é‡‡æ ·æ•°é‡
        actual_samples = min(num_samples, len(valid_positions))

        # éšæœºé‡‡æ ·ä½ç½®
        sampled_positions = sorted(random.sample(valid_positions, actual_samples))

        sampled_data = {
            'sample_positions': sampled_positions,
            'sample_tokens': [],
            'sample_logprobs': [],
            'sample_token_names': [],
            'sample_token_probs': [],
        }

        for pos in sampled_positions:
            token_id = int(token_ids[pos])

            # è·å–å®Œæ•´çš„ logits å‘é‡ï¼ˆæ‰€æœ‰ vocabï¼‰
            logit_vector = logits[0, pos, :].detach().cpu()

            # è®¡ç®—è¯¥ä½ç½®çš„ log æ¦‚ç‡å’Œæ¦‚ç‡
            log_probs = torch.nn.functional.log_softmax(logit_vector, dim=-1)
            probs = torch.softmax(logit_vector, dim=-1)

            # è·å–è¯¥ token çš„ log æ¦‚ç‡
            token_logprob = log_probs[token_id].item()
            token_prob = probs[token_id].item()

            # è·å– token åç§°
            try:
                token_name = self.tokenizer.decode([token_id]).strip()
            except:
                token_name = f"<unk_token_{token_id}>"

            sampled_data['sample_tokens'].append(token_id)
            sampled_data['sample_logprobs'].append(token_logprob)
            sampled_data['sample_token_names'].append(token_name)
            sampled_data['sample_token_probs'].append(token_prob)

        return sampled_data

    async def eval(self) -> dict:
        """æ‰§è¡Œè¯„ä¼°"""
        logger.info("Starting evaluation on datasets (transformers mode)")
        from torch.utils.data import DataLoader

        # Create dataloader
        dataloader = DataLoader(
            self.eval_dataset,
            batch_size=len(self.eval_dataset),
            shuffle=False,
            drop_last=False,
        )

        output_for_save = []
        log_dict = defaultdict(float)

        # Learning Dynamics åˆ†ææ•°æ®æ”¶é›† - æŒ‰æ•°æ®é›†åˆ†åˆ«å­˜å‚¨
        ld_by_dataset = {}  # {dataset_name: {'correct': {...}, 'incorrect': {...}, 'random': {...}}}

        # åˆå§‹åŒ–æ ·æœ¬è®¡æ•°å™¨ - æŒ‰checkpoint-datasetç»„åˆåŠ¨æ€æ”¶é›†
        all_file_names = [
            os.path.splitext(os.path.basename(file_path))[0]
            for file_path in self.cfg.eval_prompt_data
        ]
        dataset_counters = {
            dataset_name: {
                'correct': 0,
                'incorrect': 0,
                'random': 0,
                'total_evaluated': 0,
                'completed': False  # æ˜¯å¦å·²æ»¡è¶³æ¡ä»¶
            }
            for dataset_name in all_file_names
        }

        for batch in dataloader:
            prompts = list(batch[0])
            answers = list(batch[1]["answer"])
            file_names = list(batch[1]["file_name"])

            logger.info(f"Processing {len(prompts)} prompts")

            # å¯¹æ¯ä¸ª prompt è¿›è¡Œç”Ÿæˆå’Œåˆ†æ
            for sample_idx, (prompt, answer, file_name) in enumerate(zip(prompts, answers, file_names)):
                # æ£€æŸ¥è¯¥æ•°æ®é›†æ˜¯å¦å·²å®Œæˆæ”¶é›†
                if dataset_counters[file_name]['completed']:
                    logger.info(f"Skipping {file_name} sample {sample_idx + 1}/{len(prompts)} (collection completed)")
                    continue

                # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æœ€å°æ ·æœ¬æ•°é‡è¦æ±‚
                if (dataset_counters[file_name]['correct'] >= self.cfg.min_correct_samples and
                    dataset_counters[file_name]['incorrect'] >= self.cfg.min_incorrect_samples):
                    dataset_counters[file_name]['completed'] = True
                    logger.info(f"Dataset {file_name} completed: {dataset_counters[file_name]['correct']} correct, "
                                f"{dataset_counters[file_name]['incorrect']} incorrect samples collected")
                    continue

                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ä¸Šé™ï¼ˆå®‰å…¨ä¿æŠ¤ï¼‰
                if dataset_counters[file_name]['total_evaluated'] >= self.cfg.max_eval_samples:
                    dataset_counters[file_name]['completed'] = True
                    logger.warning(f"Dataset {file_name} reached max samples limit {self.cfg.max_eval_samples}")
                    continue

                logger.info(f"\n{'='*80}")
                logger.info(f"Sample {sample_idx + 1}/{len(prompts)}")
                logger.info(f"Dataset: {file_name}")
                logger.info(f"{'='*80}")

                # 1. Tokenize prompt
                prompt_inputs = self.tokenizer(prompt, return_tensors="pt", padding=True)
                prompt_input_ids = prompt_inputs["input_ids"].to(self.model.device)
                prompt_attention_mask = prompt_inputs["attention_mask"].to(self.model.device)

                # 2. Generate using transformers
                logger.info("Generating text with transformers...")
                with torch.no_grad():
                    outputs = self.model.generate(
                        input_ids=prompt_input_ids,
                        attention_mask=prompt_attention_mask,
                        max_new_tokens=self.cfg.generate_max_len,
                        temperature=self.cfg.temperature,
                        top_p=self.cfg.top_p,
                        top_k=self.cfg.top_k if self.cfg.top_k > 0 else None,
                        do_sample=self.cfg.do_sample,
                        return_dict_in_generate=False,
                        pad_token_id=self.tokenizer.eos_token_id,
                    )

                # 3. Decode output
                generated_ids = outputs[0]
                output_text = self.tokenizer.decode(generated_ids, skip_special_tokens=False)
                token_ids = generated_ids.tolist()

                logger.info(f"Generated {len(token_ids)} tokens")

                # 4. Forward pass to get complete logits
                logger.info("Getting complete logits...")
                with torch.no_grad():
                    model_outputs = self.model(
                        input_ids=generated_ids.unsqueeze(0),
                        attention_mask=torch.ones_like(generated_ids).unsqueeze(0),
                    )
                    logits = model_outputs.logits  # (1, seq_len, vocab_size)

                logger.info(f"Logits shape: {logits.shape}")

                # 5. Prepare labels for learning dynamics calculation
                prompt_len = prompt_input_ids.shape[1]
                labels = generated_ids.clone().unsqueeze(0)
                labels[:, :prompt_len] = -100  # mask æ‰ prompt éƒ¨åˆ†
                labels = labels.long()

                # 6. Extract final answer and check correctness
                pattern = re.compile(r"<answer>.*?(\\boxed{.*}).*?</answer>", re.DOTALL)
                matches = re.findall(pattern, output_text)
                final_answer = matches[-1] if len(matches) > 0 else ""

                label = solution2answer(answer)
                prefix_response = solution2answer(final_answer)
                iscorrect = await is_equal(label, prefix_response, self.executor)

                logger.info(f"Final Answer: {final_answer}")
                logger.info(f"Is Correct: {iscorrect}")

                # æ›´æ–°æ ·æœ¬è®¡æ•°å™¨
                dataset_counters[file_name]['total_evaluated'] += 1
                if iscorrect:
                    dataset_counters[file_name]['correct'] += 1
                else:
                    dataset_counters[file_name]['incorrect'] += 1

                output_for_save.append(
                    dict(
                        prompt=prompt,
                        output=output_text,
                        final_answer=final_answer,
                        answer=answer,
                        iscorrect=iscorrect,
                        token_ids=token_ids,
                    )
                )

                # ============================================================
                # Learning Dynamics åˆ†æ
                # ============================================================
                logger.info("Learning Dynamics Analysis")

                ld_metrics = None
                if labels is not None:
                    try:
                        # è°ƒç”¨åˆ†æå‡½æ•°
                        ld_result = analyze_learning_dynamics(
                            logits=logits,
                            labels=labels,
                            tokenizer=self.tokenizer,
                            verbose=False
                        )

                        # æå–å…³é”®æŒ‡æ ‡
                        ld_metrics = {
                            'out_token': ld_result['per_sample']['out_token'][0],
                            'out_argmax': ld_result['per_sample']['out_argmax'][0],
                            'A_norm': float(ld_result['per_sample']['A_norm'].squeeze()),
                            'prob_gap2_mean': ld_result['per_sample']['prob_gap2_mean'][0],
                            'prob_energy': ld_result['per_sample']['prob_energy'][0],
                        }

                        # åˆå§‹åŒ–è¯¥æ•°æ®é›†çš„ LD æ•°æ®ç»“æ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                        if file_name not in ld_by_dataset:
                            ld_by_dataset[file_name] = {
                                'correct': {
                                    'out_token': [],
                                    'out_argmax': [],
                                    'A_norm': [],
                                    'prob_gap2_mean': [],
                                    'prob_energy': [],
                                },
                                'incorrect': {
                                    'out_token': [],
                                    'out_argmax': [],
                                    'A_norm': [],
                                    'prob_gap2_mean': [],
                                    'prob_energy': [],
                                },
                                'random': {
                                    'out_token': [],
                                    'out_argmax': [],
                                    'A_norm': [],
                                    'prob_gap2_mean': [],
                                    'prob_energy': [],
                                }
                            }

                        # æ ¹æ®æ­£ç¡®æ€§åˆ†ç±»ç´¯ç§¯æ•°æ®
                        if iscorrect:
                            ld_by_dataset[file_name]['correct']['out_token'].append(ld_metrics['out_token'])
                            ld_by_dataset[file_name]['correct']['out_argmax'].append(ld_metrics['out_argmax'])
                            ld_by_dataset[file_name]['correct']['A_norm'].append(ld_metrics['A_norm'])
                            ld_by_dataset[file_name]['correct']['prob_gap2_mean'].append(ld_metrics['prob_gap2_mean'])
                            ld_by_dataset[file_name]['correct']['prob_energy'].append(ld_metrics['prob_energy'])
                        else:
                            ld_by_dataset[file_name]['incorrect']['out_token'].append(ld_metrics['out_token'])
                            ld_by_dataset[file_name]['incorrect']['out_argmax'].append(ld_metrics['out_argmax'])
                            ld_by_dataset[file_name]['incorrect']['A_norm'].append(ld_metrics['A_norm'])
                            ld_by_dataset[file_name]['incorrect']['prob_gap2_mean'].append(ld_metrics['prob_gap2_mean'])
                            ld_by_dataset[file_name]['incorrect']['prob_energy'].append(ld_metrics['prob_energy'])

                        # æ·»åŠ åˆ°ä¿å­˜çš„è¾“å‡º
                        output_for_save[-1]['ld_metrics'] = ld_metrics

                        # ========================================================
                        # éšæœºé‡‡æ · token åŠå¯¹åº”çš„ logits
                        # ========================================================
                        sampled_tokens = self._sample_random_tokens(
                            logits=logits,
                            labels=labels,
                            token_ids=token_ids,
                            num_samples=5  # éšæœºé‡‡æ · 5 ä¸ª token
                        )

                        if sampled_tokens is not None:
                            output_for_save[-1]['sampled_tokens'] = sampled_tokens

                            # å¯¹æ¯ä¸ªé‡‡æ ·ä½ç½®è®¡ç®—å­¦ä¹ åŠ¨æ€
                            for pos in sampled_tokens['sample_positions']:
                                # æ„é€ å•ä¸ªä½ç½®çš„ labelsï¼ˆå…¶ä»–ä½ç½®éƒ½ mask ä¸º -100ï¼‰
                                single_pos_labels = torch.full_like(labels, -100)
                                single_pos_labels[0, pos] = labels[0, pos]

                                try:
                                    # è®¡ç®—è¯¥ä½ç½®çš„å­¦ä¹ åŠ¨æ€
                                    ld_result_pos = analyze_learning_dynamics(
                                        logits=logits,
                                        labels=single_pos_labels,
                                        tokenizer=self.tokenizer,
                                        verbose=False
                                    )

                                    # ç´¯ç§¯åˆ°éšæœº tokens æ•°æ®ï¼ˆæŒ‰æ•°æ®é›†åˆ†åˆ«ï¼‰
                                    ld_by_dataset[file_name]['random']['out_token'].append(ld_result_pos['per_sample']['out_token'][0])
                                    ld_by_dataset[file_name]['random']['out_argmax'].append(ld_result_pos['per_sample']['out_argmax'][0])
                                    ld_by_dataset[file_name]['random']['A_norm'].append(float(ld_result_pos['per_sample']['A_norm'].squeeze()))
                                    ld_by_dataset[file_name]['random']['prob_gap2_mean'].append(ld_result_pos['per_sample']['prob_gap2_mean'][0])
                                    ld_by_dataset[file_name]['random']['prob_energy'].append(ld_result_pos['per_sample']['prob_energy'][0])

                                    # æ›´æ–°éšæœºé‡‡æ ·è®¡æ•°
                                    dataset_counters[file_name]['random'] += 1

                                except Exception as e:
                                    logger.warning(f"Failed to compute learning dynamics for sampled token at position {pos}: {e}")

                        # æ£€æŸ¥è¯¥æ•°æ®é›†æ˜¯å¦å·²æ»¡è¶³æ‰€æœ‰æ¡ä»¶
                        counter = dataset_counters[file_name]
                        if (counter['correct'] >= self.cfg.min_correct_samples and
                            counter['incorrect'] >= self.cfg.min_incorrect_samples and
                            counter['random'] >= self.cfg.min_random_samples):
                            dataset_counters[file_name]['completed'] = True
                            logger.info(f"âœ… {file_name}: æ‰€æœ‰ç±»åˆ«å·²æ»¡è¶³æœ€å°æ ·æœ¬æ•°è¦æ±‚")
                            logger.info(f"   âœ“ æ­£ç¡®ç­”æ¡ˆ: {counter['correct']}/{self.cfg.min_correct_samples}")
                            logger.info(f"   âœ“ é”™è¯¯ç­”æ¡ˆ: {counter['incorrect']}/{self.cfg.min_incorrect_samples}")
                            logger.info(f"   âœ“ éšæœºtoken: {counter['random']}/{self.cfg.min_random_samples}")

                    except Exception as e:
                        logger.warning(f"Failed to compute learning dynamics: {e}")

                logger.info(f"{'='*80}\n")

        # Calculate metrics per dataset
        all_file_names = [
            os.path.splitext(os.path.basename(file_path))[0]
            for file_path in self.cfg.eval_prompt_data
        ]

        for file_name in all_file_names:
            if log_dict[f"{file_name}/total"] > 0:
                log_dict[f"{file_name}/response_len_in_char"] = (
                    log_dict[f"{file_name}/total_response_len_in_char"]
                    / log_dict[f"{file_name}/total"]
                )
                log_dict[f"{file_name}/accuracy"] = (
                    log_dict[f"{file_name}/correct"] / log_dict[f"{file_name}/total"]
                )
                log_dict.pop(f"{file_name}/total_response_len_in_char")
                log_dict.pop(f"{file_name}/correct")
                log_dict.pop(f"{file_name}/total")

        # Calculate average accuracy
        accuracies = [log_dict[f"{fn}/accuracy"] for fn in all_file_names if f"{fn}/accuracy" in log_dict]
        if accuracies:
            log_dict["eval_accuracy"] = sum(accuracies) / len(accuracies)

        # ====================================================================
        # è®¡ç®— Learning Dynamics ç»Ÿè®¡ - æŒ‰æ•°æ®é›†åˆ†åˆ«
        # ====================================================================
        for dataset_name, dataset_ld in ld_by_dataset.items():
            # æ­£ç¡®ç­”æ¡ˆç»Ÿè®¡
            if dataset_ld['correct']['out_token']:
                log_dict[f"{dataset_name}/ld_correct/out_token"] = float(np.mean(dataset_ld['correct']['out_token']))
                log_dict[f"{dataset_name}/ld_correct/out_argmax"] = float(np.mean(dataset_ld['correct']['out_argmax']))
                log_dict[f"{dataset_name}/ld_correct/A_norm"] = float(np.mean(dataset_ld['correct']['A_norm']))
                log_dict[f"{dataset_name}/ld_correct/prob_gap2_mean"] = float(np.mean(dataset_ld['correct']['prob_gap2_mean']))
                log_dict[f"{dataset_name}/ld_correct/prob_energy"] = float(np.mean(dataset_ld['correct']['prob_energy']))
                log_dict[f"{dataset_name}/ld_correct/count"] = len(dataset_ld['correct']['out_token'])

            # é”™è¯¯ç­”æ¡ˆç»Ÿè®¡
            if dataset_ld['incorrect']['out_token']:
                log_dict[f"{dataset_name}/ld_incorrect/out_token"] = float(np.mean(dataset_ld['incorrect']['out_token']))
                log_dict[f"{dataset_name}/ld_incorrect/out_argmax"] = float(np.mean(dataset_ld['incorrect']['out_argmax']))
                log_dict[f"{dataset_name}/ld_incorrect/A_norm"] = float(np.mean(dataset_ld['incorrect']['A_norm']))
                log_dict[f"{dataset_name}/ld_incorrect/prob_gap2_mean"] = float(np.mean(dataset_ld['incorrect']['prob_gap2_mean']))
                log_dict[f"{dataset_name}/ld_incorrect/prob_energy"] = float(np.mean(dataset_ld['incorrect']['prob_energy']))
                log_dict[f"{dataset_name}/ld_incorrect/count"] = len(dataset_ld['incorrect']['out_token'])

            # éšæœºé‡‡æ ·Tokenç»Ÿè®¡
            if dataset_ld['random']['out_token']:
                log_dict[f"{dataset_name}/ld_random/out_token"] = float(np.mean(dataset_ld['random']['out_token']))
                log_dict[f"{dataset_name}/ld_random/out_argmax"] = float(np.mean(dataset_ld['random']['out_argmax']))
                log_dict[f"{dataset_name}/ld_random/A_norm"] = float(np.mean(dataset_ld['random']['A_norm']))
                log_dict[f"{dataset_name}/ld_random/prob_gap2_mean"] = float(np.mean(dataset_ld['random']['prob_gap2_mean']))
                log_dict[f"{dataset_name}/ld_random/prob_energy"] = float(np.mean(dataset_ld['random']['prob_energy']))
                log_dict[f"{dataset_name}/ld_random/count"] = len(dataset_ld['random']['out_token'])

        # ====================================================================
        # æ ·æœ¬æ”¶é›†ç»Ÿè®¡æŠ¥å‘Š
        # ====================================================================
        logger.info("\n" + "="*80)
        logger.info("ã€æ ·æœ¬æ”¶é›†ç»Ÿè®¡æŠ¥å‘Šã€‘")
        logger.info("="*80)
        for dataset_name in all_file_names:
            counter = dataset_counters[dataset_name]
            logger.info(f"\nã€{dataset_name}ã€‘")
            logger.info(f"  å·²è¯„ä¼°æ ·æœ¬æ•°: {counter['total_evaluated']}/{self.cfg.max_eval_samples}")
            logger.info(f"  æ­£ç¡®ç­”æ¡ˆ: {counter['correct']}/{self.cfg.min_correct_samples}", end="")
            if counter['correct'] >= self.cfg.min_correct_samples:
                logger.info(" âœ…")
            else:
                logger.info(" âš ï¸ (ä¸è¶³)")

            logger.info(f"  é”™è¯¯ç­”æ¡ˆ: {counter['incorrect']}/{self.cfg.min_incorrect_samples}", end="")
            if counter['incorrect'] >= self.cfg.min_incorrect_samples:
                logger.info(" âœ…")
            else:
                logger.info(" âš ï¸ (ä¸è¶³)")

            logger.info(f"  éšæœºtoken: {counter['random']}/{self.cfg.min_random_samples}", end="")
            if counter['random'] >= self.cfg.min_random_samples:
                logger.info(" âœ…")
            else:
                logger.info(" âš ï¸ (ä¸è¶³)")

            # åˆ¤æ–­æ˜¯å¦å¯ä¿¡
            if (counter['correct'] >= self.cfg.min_correct_samples and
                counter['incorrect'] >= self.cfg.min_incorrect_samples and
                counter['random'] >= self.cfg.min_random_samples):
                logger.info(f"  ğŸ“Š æ ·æœ¬æ•°å……è¶³ï¼Œç»Ÿè®¡ç»“æœå¯ä¿¡")
            else:
                logger.warning(f"  âš ï¸ éƒ¨åˆ†ç±»åˆ«æ ·æœ¬ä¸è¶³ï¼Œç»Ÿè®¡ç»“æœä»…ä¾›å‚è€ƒ")

        logger.info("="*80 + "\n")

        # Output Learning Dynamics summary by dataset
        for dataset_name, dataset_ld in ld_by_dataset.items():
            if dataset_ld['correct']['out_token'] or dataset_ld['incorrect']['out_token']:
                logger.info(f"\n={'='*80}")
                logger.info(f"ã€{dataset_name} - Learning Dynamics åˆ†ææ‘˜è¦ã€‘")
                logger.info(f"{'='*80}")

                if dataset_ld['correct']['out_token']:
                    logger.info(f"Correct answers (n={len(dataset_ld['correct']['out_token'])})")
                    logger.info(f"  Energy (prob_energy): {log_dict[f'{dataset_name}/ld_correct/prob_energy']:.4f}")
                    logger.info(f"  Gap (prob_gap2_mean): {log_dict[f'{dataset_name}/ld_correct/prob_gap2_mean']:.4f}")
                    logger.info(f"  A_norm: {log_dict[f'{dataset_name}/ld_correct/A_norm']:.4f}")
                    logger.info(f"  out_token: {log_dict[f'{dataset_name}/ld_correct/out_token']:.4f}")
                    logger.info(f"  out_argmax: {log_dict[f'{dataset_name}/ld_correct/out_argmax']:.4f}")

                if dataset_ld['incorrect']['out_token']:
                    logger.info(f"Incorrect answers (n={len(dataset_ld['incorrect']['out_token'])})")
                    logger.info(f"  Energy (prob_energy): {log_dict[f'{dataset_name}/ld_incorrect/prob_energy']:.4f}")
                    logger.info(f"  Gap (prob_gap2_mean): {log_dict[f'{dataset_name}/ld_incorrect/prob_gap2_mean']:.4f}")
                    logger.info(f"  A_norm: {log_dict[f'{dataset_name}/ld_incorrect/A_norm']:.4f}")
                    logger.info(f"  out_token: {log_dict[f'{dataset_name}/ld_incorrect/out_token']:.4f}")
                    logger.info(f"  out_argmax: {log_dict[f'{dataset_name}/ld_incorrect/out_argmax']:.4f}")

                if dataset_ld['correct']['out_token'] and dataset_ld['incorrect']['out_token']:
                    logger.info("Comparison")
                    energy_diff = log_dict[f'{dataset_name}/ld_incorrect/prob_energy'] - log_dict[f'{dataset_name}/ld_correct/prob_energy']
                    gap_diff = log_dict[f'{dataset_name}/ld_incorrect/prob_gap2_mean'] - log_dict[f'{dataset_name}/ld_correct/prob_gap2_mean']
                    logger.info(f"  Energy difference (incorrect - correct): {energy_diff:.4f}")
                    logger.info(f"  Gap difference (incorrect - correct): {gap_diff:.4f}")
                    if energy_diff > 0:
                        logger.info("  Incorrect answers need more learning energy (model less confident)")
                    else:
                        logger.info("  Correct answers need more learning energy")

                if dataset_ld['random']['out_token']:
                    logger.info(f"Random sampled tokens (n={len(dataset_ld['random']['out_token'])})")
                    logger.info(f"  Energy (prob_energy): {log_dict[f'{dataset_name}/ld_random/prob_energy']:.4f}")
                    logger.info(f"  Gap (prob_gap2_mean): {log_dict[f'{dataset_name}/ld_random/prob_gap2_mean']:.4f}")
                    logger.info(f"  A_norm: {log_dict[f'{dataset_name}/ld_random/A_norm']:.4f}")

        # Generate Learning Dynamics visualization - one chart per dataset
        if self.cfg.enable_visualization:
            for dataset_name, dataset_ld in ld_by_dataset.items():
                if dataset_ld['correct']['out_token'] or dataset_ld['incorrect']['out_token']:
                    try:
                        figure_path = self._visualize_learning_dynamics(
                            dataset_ld['correct'],
                            dataset_ld['incorrect'],
                            dataset_ld['random'],
                            dataset_name=dataset_name
                        )
                        if figure_path:
                            logger.info(f"Visualization generated for {dataset_name}: {figure_path}")
                    except Exception as e:
                        logger.warning(f"Failed to generate {dataset_name} visualization: {e}")

        # Save results if requested
        if self.cfg.save_detailed_results:
            os.makedirs(self.cfg.output_dir, exist_ok=True)

            # Generate result filename
            dump_file_name = "eval_results"
            for file_name in all_file_names:
                if f"{file_name}/accuracy" in log_dict:
                    dump_file_name += f"_{file_name}_{log_dict[f'{file_name}/accuracy']:.4f}"
            dump_file_name += ".jsonl"

            result_path = os.path.join(self.cfg.output_dir, dump_file_name)
            logger.info(f"Saving evaluation results to {result_path}")
            with open(result_path, "w") as f:
                for item in output_for_save:
                    # é€’å½’è½¬æ¢æ‰€æœ‰ torch å’Œ numpy ç±»å‹ä¸º Python åŸç”Ÿç±»å‹
                    def convert_to_serializable(obj):
                        if isinstance(obj, dict):
                            return {k: convert_to_serializable(v) for k, v in obj.items()}
                        elif isinstance(obj, list):
                            return [convert_to_serializable(v) for v in obj]
                        elif isinstance(obj, torch.Tensor):
                            return obj.detach().cpu().tolist()
                        elif isinstance(obj, np.ndarray):
                            return obj.tolist()
                        elif isinstance(obj, (np.integer, np.floating)):
                            return float(obj)
                        elif isinstance(obj, bool):
                            return bool(obj)
                        elif isinstance(obj, (int, float, str)):
                            return obj
                        else:
                            # Handle torch scalar types
                            try:
                                return float(obj)
                            except (TypeError, ValueError):
                                return str(obj)

                    serializable_item = convert_to_serializable(item)
                    f.write(json.dumps(serializable_item, ensure_ascii=False) + "\n")

        # Log results
        logging_str = ",".join([f"{k}: {v:.4f}" for k, v in log_dict.items()])
        logger.info(f"Evaluation completed: {logging_str}")

        # ====================================================================
        # è‡ªå®šä¹‰è¯­å¥åˆ†æï¼ˆå¦‚æœæä¾›ï¼‰
        # ====================================================================
        if self.cfg.custom_sentence_files is not None and len(self.cfg.custom_sentence_files) > 0:
            try:
                logger.info("\n" + "="*80)
                logger.info("ã€Custom Sentence Analysisã€‘")
                logger.info("="*80)
                ld_custom_sentences = await self._analyze_custom_sentence()

                # ç»Ÿè®¡è‡ªå®šä¹‰å¥å­çš„å­¦ä¹ åŠ¨æ€æŒ‡æ ‡
                if ld_custom_sentences:
                    for file_name, ld_metrics in ld_custom_sentences.items():
                        log_dict[f"custom_sentence_{file_name}/out_token"] = ld_metrics['out_token']
                        log_dict[f"custom_sentence_{file_name}/out_argmax"] = ld_metrics['out_argmax']
                        log_dict[f"custom_sentence_{file_name}/A_norm"] = ld_metrics['A_norm']
                        log_dict[f"custom_sentence_{file_name}/prob_gap2_mean"] = ld_metrics['prob_gap2_mean']
                        log_dict[f"custom_sentence_{file_name}/prob_energy"] = ld_metrics['prob_energy']

                    # è¾“å‡ºè‡ªå®šä¹‰å¥å­ç»Ÿè®¡æ€»ç»“
                    logger.info("\n" + "="*80)
                    logger.info("ã€Custom Sentence Learning Dynamics Summaryã€‘")
                    logger.info("="*80 + "\n")
                    for file_name, ld_metrics in ld_custom_sentences.items():
                        logger.info(f"\nã€{file_name}ã€‘")
                        logger.info(f"  out_token: {ld_metrics['out_token']:.4f}")
                        logger.info(f"  out_argmax: {ld_metrics['out_argmax']:.4f}")
                        logger.info(f"  A_norm: {ld_metrics['A_norm']:.4f}")
                        logger.info(f"  prob_gap2_mean: {ld_metrics['prob_gap2_mean']:.4f}")
                        logger.info(f"  prob_energy: {ld_metrics['prob_energy']:.4f}")
                    logger.info("\n" + "="*80 + "\n")

            except Exception as e:
                logger.warning(f"Failed to analyze custom sentence: {e}")
                import traceback
                traceback.print_exc()

        return dict(log_dict)

    def _sample_random_tokens(self, logits, labels, token_ids, num_samples=5):
        """
        ä»ç”Ÿæˆçš„åºåˆ—ä¸­éšæœºé‡‡æ · token åŠå…¶å¯¹åº”çš„ logits

        Args:
            logits: torch.FloatTensor, shape (1, seq_len, vocab_size)
            labels: torch.LongTensor, shape (1, seq_len)
            token_ids: list of generated token IDs
            num_samples: é‡‡æ ·çš„ token æ•°é‡ï¼ˆé»˜è®¤ 5ï¼‰

        Returns:
            sampled_data: å­—å…¸ï¼ŒåŒ…å«é‡‡æ ·çš„ token ä¿¡æ¯
                {
                    'sample_positions': [pos1, pos2, ...],
                    'sample_tokens': [token_id1, token_id2, ...],
                    'sample_logits': [logits_vector_1, logits_vector_2, ...],  # æ¯ä¸ªéƒ½æ˜¯å®Œæ•´çš„ vocab_size ç»´åº¦
                    'sample_token_names': ['token_name1', 'token_name2', ...],
                }
        """
        import random

        if logits is None or labels is None or token_ids is None:
            return None

        seq_len = len(token_ids)

        # æœ‰æ•ˆä½ç½®æ•°ï¼ˆé -100 maskï¼‰
        valid_positions = [i for i in range(seq_len) if i < labels.shape[1] and labels[0, i] != -100]

        if len(valid_positions) == 0:
            return None

        # ç¡®å®šå®é™…é‡‡æ ·æ•°é‡
        actual_samples = min(num_samples, len(valid_positions))

        # éšæœºé‡‡æ ·ä½ç½®
        sampled_positions = sorted(random.sample(valid_positions, actual_samples))

        sampled_data = {
            'sample_positions': sampled_positions,
            'sample_tokens': [],
            'sample_logits': [],
            'sample_logprobs': [],  # log æ¦‚ç‡
            'sample_token_names': [],
            'sample_token_probs': [],  # å®é™…æ¦‚ç‡
        }

        for pos in sampled_positions:
            token_id = int(token_ids[pos])

            # è·å–å®Œæ•´çš„ logits å‘é‡ï¼ˆæ‰€æœ‰ vocabï¼‰
            logit_vector = logits[0, pos, :].detach().cpu()

            # è®¡ç®—è¯¥ä½ç½®çš„ log æ¦‚ç‡å’Œæ¦‚ç‡
            log_probs = torch.nn.functional.log_softmax(logit_vector, dim=-1)
            probs = torch.softmax(logit_vector, dim=-1)

            # è·å–è¯¥ token çš„ log æ¦‚ç‡
            token_logprob = log_probs[token_id].item()
            token_prob = probs[token_id].item()

            # è·å– token åç§°
            try:
                token_name = self.tokenizer.decode([token_id]).strip()
            except:
                token_name = f"<unk_token_{token_id}>"

            sampled_data['sample_tokens'].append(token_id)
            sampled_data['sample_logits'].append(logit_vector.numpy().tolist())  # å®Œæ•´ logits å‘é‡
            sampled_data['sample_logprobs'].append(token_logprob)
            sampled_data['sample_token_names'].append(token_name)
            sampled_data['sample_token_probs'].append(token_prob)

        return sampled_data

    def _visualize_learning_dynamics(self, ld_correct_samples, ld_incorrect_samples, ld_random_tokens=None, dataset_name="All"):
        """
        Generate comparison bar chart for learning dynamics metrics

        Args:
            ld_correct_samples: æ­£ç¡®ç­”æ¡ˆçš„å­¦ä¹ åŠ¨æ€æ•°æ®
            ld_incorrect_samples: é”™è¯¯ç­”æ¡ˆçš„å­¦ä¹ åŠ¨æ€æ•°æ®
            ld_random_tokens: éšæœºé‡‡æ ·tokençš„å­¦ä¹ åŠ¨æ€æ•°æ®ï¼ˆå¯é€‰ï¼‰
            dataset_name: æ•°æ®é›†åç§°ï¼Œç”¨äºå›¾è¡¨æ ‡é¢˜å’Œæ–‡ä»¶å
        """
        # Check if there is data
        if not ld_correct_samples['out_token'] and not ld_incorrect_samples['out_token']:
            logger.warning(f"No learning dynamics data to visualize for {dataset_name}")
            return None

        # 5 key metrics
        metrics = ['out_token', 'out_argmax', 'A_norm', 'prob_gap2_mean', 'prob_energy']
        metric_labels = {
            'out_token': 'True Label\nLog Probability',
            'out_argmax': 'Argmax Token\nLog Probability',
            'A_norm': 'Output Vector\nNorm',
            'prob_gap2_mean': 'Prediction-Label\nGap',
            'prob_energy': 'Pull-up Energy\n(Correction Strength)',
        }

        # Calculate mean and std for each metric
        correct_means = []
        correct_stds = []
        incorrect_means = []
        incorrect_stds = []
        random_means = []
        random_stds = []

        for metric in metrics:
            # Correct answers
            if ld_correct_samples[metric]:
                correct_means.append(float(np.mean(ld_correct_samples[metric])))
                correct_stds.append(float(np.std(ld_correct_samples[metric])))
            else:
                correct_means.append(0)
                correct_stds.append(0)

            # Incorrect answers
            if ld_incorrect_samples[metric]:
                incorrect_means.append(float(np.mean(ld_incorrect_samples[metric])))
                incorrect_stds.append(float(np.std(ld_incorrect_samples[metric])))
            else:
                incorrect_means.append(0)
                incorrect_stds.append(0)

            # Random tokensï¼ˆå¯é€‰ï¼‰
            if ld_random_tokens and ld_random_tokens[metric]:
                random_means.append(float(np.mean(ld_random_tokens[metric])))
                random_stds.append(float(np.std(ld_random_tokens[metric])))
            else:
                random_means.append(0)
                random_stds.append(0)

        # Create figure
        fig, ax = plt.subplots(figsize=(16, 8))

        x = np.arange(len(metrics))

        # è°ƒæ•´å®½åº¦ä»¥æ”¯æŒ 3 ç»„æ•°æ®
        if ld_random_tokens and ld_random_tokens['out_token']:
            width = 0.25  # 3 ç»„æŸ±å­
            bars1 = ax.bar(x - width, correct_means, width, label='Correct Answers',
                           color='#2ecc71', alpha=0.8, edgecolor='black', linewidth=1.5,
                           yerr=correct_stds, capsize=5, error_kw={'linewidth': 1.5})
            bars2 = ax.bar(x, incorrect_means, width, label='Incorrect Answers',
                           color='#e74c3c', alpha=0.8, edgecolor='black', linewidth=1.5,
                           yerr=incorrect_stds, capsize=5, error_kw={'linewidth': 1.5})
            bars3 = ax.bar(x + width, random_means, width, label='Random Tokens',
                           color='#3498db', alpha=0.8, edgecolor='black', linewidth=1.5,
                           yerr=random_stds, capsize=5, error_kw={'linewidth': 1.5})

            def add_value_labels(bars):
                for bar in bars:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                           f'{height:.3f}',
                           ha='center', va='bottom', fontsize=9, fontweight='bold')

            add_value_labels(bars1)
            add_value_labels(bars2)
            add_value_labels(bars3)
        else:
            # åªæœ‰ 2 ç»„æ•°æ®
            width = 0.35
            bars1 = ax.bar(x - width/2, correct_means, width, label='Correct Answers',
                           color='#2ecc71', alpha=0.8, edgecolor='black', linewidth=1.5,
                           yerr=correct_stds, capsize=5, error_kw={'linewidth': 1.5})
            bars2 = ax.bar(x + width/2, incorrect_means, width, label='Incorrect Answers',
                           color='#e74c3c', alpha=0.8, edgecolor='black', linewidth=1.5,
                           yerr=incorrect_stds, capsize=5, error_kw={'linewidth': 1.5})

            def add_value_labels(bars):
                for bar in bars:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                           f'{height:.3f}',
                           ha='center', va='bottom', fontsize=10, fontweight='bold')

            add_value_labels(bars1)
            add_value_labels(bars2)

        # Set axes
        ax.set_xlabel('Learning Dynamics Metrics', fontsize=14, fontweight='bold')
        ax.set_ylabel('Metric Value', fontsize=14, fontweight='bold')
        ax.set_title(f'Learning Dynamics Comparison - {dataset_name}', fontsize=16, fontweight='bold', pad=20)
        ax.set_xticks(x)
        ax.set_xticklabels([metric_labels[m] for m in metrics], fontsize=11)
        ax.legend(fontsize=12, loc='upper left')
        ax.grid(axis='y', alpha=0.3, linestyle='--')

        # Add sample count info
        correct_count = len(ld_correct_samples['out_token']) if ld_correct_samples['out_token'] else 0
        incorrect_count = len(ld_incorrect_samples['out_token']) if ld_incorrect_samples['out_token'] else 0
        random_count = len(ld_random_tokens['out_token']) if ld_random_tokens and ld_random_tokens['out_token'] else 0

        if random_count > 0:
            info_text = f'Correct Samples: {correct_count}  |  Incorrect Samples: {incorrect_count}  |  Random Tokens: {random_count}'
        else:
            info_text = f'Correct Samples: {correct_count}  |  Incorrect Samples: {incorrect_count}'

        fig.text(0.5, 0.02, info_text, ha='center', fontsize=11, style='italic')

        # Tight layout
        plt.tight_layout(rect=[0, 0.03, 1, 1])

        # Save figure with dataset name to figures subdirectory
        figures_dir = os.path.join(self.cfg.output_dir, "figures")
        os.makedirs(figures_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        figure_path = os.path.join(figures_dir, f"ld_comparison_{dataset_name}_{timestamp}.png")
        plt.savefig(figure_path, dpi=150, bbox_inches='tight')
        plt.close()

        logger.info(f"Learning Dynamics visualization saved to: {figure_path}")
        return figure_path


    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("Cleaning up evaluator resources")
        if self.model is not None:
            del self.model
            torch.cuda.empty_cache()
        if ray.is_initialized():
            ray.shutdown()
        logger.info("Cleanup completed")


if __name__ == "__main__":
    import argparse

    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Evaluate model checkpoints with learning dynamics analysis")
    parser.add_argument("--max_eval_samples", type=int, default=5000,
                        help="Max number of samples to evaluate per dataset (default: 5000)")
    parser.add_argument("--min_correct_samples", type=int, default=30,
                        help="Minimum number of correct samples per dataset-checkpoint (default: 30)")
    parser.add_argument("--min_incorrect_samples", type=int, default=10,
                        help="Minimum number of incorrect samples per dataset-checkpoint (default: 10)")
    parser.add_argument("--min_random_samples", type=int, default=10,
                        help="Minimum number of random tokens per dataset-checkpoint (default: 10)")
    parser.add_argument("--checkpoint_paths", nargs="+", default=[
        "/mnt/shared-storage-user/tangjia/orz_7b_ppo_jericho_1013/iter0/policy",
        "/mnt/shared-storage-user/tangjia/orz_7b_ppo_jericho_1013/iter45/policy",
        "/mnt/shared-storage-user/tangjia/orz_7b_ppo_jericho_1013/iter90/policy",
        "/mnt/shared-storage-user/tangjia/orz_7b_ppo_jericho_1013/iter180/policy",
    ], help="Checkpoint paths to evaluate (default: iter0, iter45, iter90, iter180)")
    parser.add_argument("--output_dir", type=str, default="orz_dynamic_log",
                        help="Output directory for results (default: orz_dynamic_log)")
    parser.add_argument("--log_dir", type=str, default="orz_dynamic_log",
                        help="Log directory name (default: orz_dynamic_log)")
    parser.add_argument("--generate_max_len", type=int, default=8000,
                        help="Maximum generation length (default: 8000)")
    parser.add_argument("--temperature", type=float, default=1.0,
                        help="Sampling temperature (default: 1.0)")
    parser.add_argument("--top_p", type=float, default=1.0,
                        help="Top-p sampling parameter (default: 1.0)")
    parser.add_argument("--top_k", type=int, default=-1,
                        help="Top-k sampling parameter (default: -1, disabled)")

    args = parser.parse_args()

    # Configure logging with custom log directory
    log_dir = args.log_dir
    os.makedirs(log_dir, exist_ok=True)
    log_date = datetime.now().strftime("%Y%m%d")
    logger.add(
        os.path.join(log_dir, f"eval_{log_date}.log"),
        rotation="00:00",
        retention="30 days",
        level="INFO",
        encoding="utf-8",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}"
    )

    logger.info("Running in evaluation mode (transformers)")
    logger.info(f"Command line arguments: {args}")

    checkpoint_path_list = args.checkpoint_paths
    all_model_results = {}

    for checkpoint_path in checkpoint_path_list:
        logger.info(f"\n{'='*80}")
        logger.info(f"ã€Processing: {checkpoint_path}ã€‘")
        logger.info(f"{'='*80}\n")

        eval_config = EvaluatorConfig(
            model_path=checkpoint_path,
            tokenizer_path=checkpoint_path,
            temperature=args.temperature,
            top_p=args.top_p,
            top_k=args.top_k,
            generate_max_len=args.generate_max_len,
            eval_prompt_data=[
                "data/eval_data/eval_jericho_dataset_his10_4games_1.8k_20251013_instruct.json",
                "data/eval_data/math500.json",
                "data/eval_data/aime2024.json",
                "data/eval_data/gpqa_diamond.json",
            ],
            prompt_max_len=2048,
            output_dir=args.output_dir,
            log_dir=args.log_dir,
            save_detailed_results=True,
            max_eval_samples=args.max_eval_samples,
            min_correct_samples=args.min_correct_samples,
            min_incorrect_samples=args.min_incorrect_samples,
            min_random_samples=args.min_random_samples,
        )
        evaluator = Evaluator(eval_config)

        try:
            results = asyncio.run(evaluator.eval())
            logger.info(f"Evaluation results for {checkpoint_path}: {results}")
            all_model_results[checkpoint_path] = results
        finally:
            evaluator.cleanup()

    # ====================================================================
    # ã€å¤š Checkpoint è¶‹åŠ¿åˆ†æå’Œå¯è§†åŒ–ã€‘
    # ====================================================================
    logger.info("Multi-Checkpoint Learning Dynamics Trend Analysis")

    logger.info("Aggregating results from multiple checkpoints...")
    aggregated_data = aggregate_multi_checkpoint_results(all_model_results)

    logger.info("Aggregated data statistics:")
    for dataset_name, dataset_data in aggregated_data.items():
        logger.info(f"  Dataset: {dataset_name}")
        for sample_type in ['correct', 'incorrect', 'random']:
            if sample_type in dataset_data and dataset_data[sample_type]:
                num_metrics = len(dataset_data[sample_type])
                logger.info(f"    {sample_type}: {num_metrics} metrics")

    logger.info("Generating multi-checkpoint trends visualization...")
    visualize_multi_checkpoint_trends(aggregated_data, output_dir="orz_dynamic_log")

    logger.info("All evaluations and visualizations completed!")
